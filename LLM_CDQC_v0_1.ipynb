{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waynelee9511cloud/my-colab-notebooks/blob/main/LLM_CDQC_v0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 第一步：安裝環境與指定模型\n",
        "\n",
        "!pip -q install -U \"pandas==2.2.2\"\n",
        "!pip -q install -U \"transformers>=4.50.0\" accelerate huggingface_hub openpyxl \"pillow<12.0\" einops safetensors tqdm trl peft datasets\n",
        "\n",
        "import os, json, re, time\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ===========================\n",
        "# 1) 基本設定\n",
        "# ===========================\n",
        "#MODEL_ID = \"BioMistral/BioMistral-7B\"  # BioMistral 7B model\n",
        "#MODEL_ID = \"WayneLee9511/WL-BMS-SFT-1400-1020-40epochs\"  # Fine tune model: BioMistral_sft_out/final\n",
        "MODEL_ID = \"google/medgemma-4b-it\"\n",
        "#MODEL_ID = \"WayneLee9511/medgemma-4b-it-sft-lora-crc100k\" #medgemma fine tune\n",
        "EXCEL_PATH = None  # 直接填入檔案路徑字串；若為 None 會開啟上傳視窗\n",
        "OUTPUT_CSV = \"validation_results.csv\"\n",
        "MAX_ROWS = None  # 例如 500，或設 None 全部處理\n",
        "#DO_FINETUNE = True  # 若要微調改成 True（需自備/產生訓練資料）\n",
        "DO_FINETUNE = False  # 若要微調改成 True（需自備/產生訓練資料）\n",
        "\n",
        "\n",
        "PROMPT_CONFIG = {\n",
        "    \"MH\": {\n",
        "        \"target_column\": \"[Diagnosis/ Conditions]\",\n",
        "        \"error_type\": \"Incorrect diagnosis name\",\n",
        "        \"example_error\": \"[Mypia] should be [Myopia].\",\n",
        "        \"example_correct\": \"Hypertension\"\n",
        "    },\n",
        "    \"CM\": {\n",
        "        \"target_column\": \"[Generic/Brand Name]\",\n",
        "        \"error_type\": \"Incorrect drug name\",\n",
        "        \"example_error\": \"[Lisnopril] should be [Lisinopril].\",\n",
        "        \"example_correct\": \"Aspirin 100mg\"\n",
        "    },\n",
        "    \"AE\": {\n",
        "        \"target_column\": \"[Event]\",\n",
        "        \"error_type\": \"Incorrect event name\",\n",
        "        \"example_error\": \"[Headchae] should be [Headache].\",\n",
        "        \"example_correct\": \"Nausea\"\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT_ZH = (\n",
        "    \"You are a specialized tool for identifying spelling errors in specific medical data fields.\"\n",
        "    \"Your ONLY task is to check for spelling errors in the values of the following three fields: '[Diagnosis/ Conditions]', '[Event]', and '[Generic/Brand Name]'.\",\n",
        "    \"You MUST IGNORE all other fields, such as IDs, dates, flags, sequence numbers, or any other columns. Do not report any issues for these other fields.\",\n",
        "    \"If you find a spelling error, your reason MUST be specific and follow the format from the examples below. If there are no spelling errors in the specified fields, your summary must be 'No issues found.'.\",\n",
        "    \"Your entire response MUST be a single, valid JSON object and nothing else.\",\n",
        "\n",
        "    # --- Example 1: Data with a diagnosis spelling error ---\n",
        "    \"Example Input Data:\\n\"\n",
        "    \"[Diagnosis/ Conditions]: Mypia\\n\",\n",
        "\n",
        "    \"Example JSON Output:\\n\"\n",
        "    '{\\n'\n",
        "    '  \"row_index\": 0,\\n'\n",
        "    '  \"issues\": [\\n'\n",
        "    '    {\\n'\n",
        "    '      \"field\": \"[Diagnosis/ Conditions]\",\\n'\n",
        "    '      \"reason\": \"Incorrect diagnosis name: [Mypia] should be [Myopia].\",\\n'\n",
        "    '      \"severity\": \"error\"\\n'\n",
        "    '    }\\n'\n",
        "    '  ],\\n'\n",
        "    '  \"summary\": \"Incorrect diagnosis name: [Mypia] should be [Myopia].\"\\n'\n",
        "    '}',\n",
        "\n",
        "    # --- Example 2: Data with a drug spelling error ---\n",
        "    \"Example Input Data:\\n\"\n",
        "    \"[Generic/Brand Name]: Lisnopril 10mg\\n\",\n",
        "\n",
        "    \"Example JSON Output:\\n\"\n",
        "    '{\\n'\n",
        "    '  \"row_index\": 1,\\n'\n",
        "    '  \"issues\": [\\n'\n",
        "    '    {\\n'\n",
        "    '      \"field\": \"[Generic/Brand Name]\",\\n'\n",
        "    '      \"reason\": \"Incorrect drug name: [Lisnopril] should be [Lisinopril].\",\\n'\n",
        "    '      \"severity\": \"error\"\\n'\n",
        "    '    }\\n'\n",
        "    '  ],\\n'\n",
        "    '  \"summary\": \"Incorrect drug name: [Lisnopril] should be [Lisinopril].\"\\n'\n",
        "    '}',\n",
        "\n",
        "    # --- Example 3: Data with no spelling issues ---\n",
        "    \"Example Input Data:\\n\"\n",
        "    \"[Diagnosis/ Conditions]: Hypertension\\n\",\n",
        "\n",
        "    \"Example JSON Output:\\n\"\n",
        "    '{\\n'\n",
        "    '  \"row_index\": 2,\\n'\n",
        "    '  \"issues\": [],\\n'\n",
        "    '  \"summary\": \"No issues found.\"\\n'\n",
        "    '}'\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Packages installed successfully. Ready to load model.\")"
      ],
      "metadata": {
        "id": "-ruGcKzbtSYu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  第二步：將模型掛載於google drive方便日後快速讀取\n",
        "\n",
        "# ===========================\n",
        "# 3) 載入模型與處理器（bf16 + 自動放到 GPU）\n",
        "# - 使用 dtype 取代舊參數 torch_dtype\n",
        "# ===========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ['HF_HOME'] = '/content/drive/MyDrive/LLMcache6'\n",
        "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/LLMcache6'\n",
        "\n",
        "\n",
        "#MODEL_ID = \"BioMistral/BioMistral-7B\"\n",
        "#MODEL_ID = \"WayneLee9511/WL-BMS-SFT-1400-1020-40epochs\"\n",
        "MODEL_ID = \"google/medgemma-4b-it\"\n",
        "#MODEL_ID = \"WayneLee9511/medgemma-4b-it-sft-lora-crc100k\" #medgemma fine tune\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 載入 tokenizer 並修正 padding token 問題\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "processor = tokenizer\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4PJ_kxb6tmJ8",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjoGSG1ofQne",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title  第三步：模型會跳出上傳按鍵，於讀Excel後產結果\n",
        "\n",
        "from google.colab import files\n",
        "print(\"請上傳 xlsx 檔案...\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise RuntimeError(\"未上傳任何檔案\")\n",
        "EXCEL_PATH = next(iter(uploaded))\n",
        "print(\"已選擇檔案：\", EXCEL_PATH)\n",
        "\n",
        "sheets = pd.read_excel(EXCEL_PATH, sheet_name=None, engine=\"openpyxl\")\n",
        "if MAX_ROWS is not None:\n",
        "    for k in list(sheets.keys()):\n",
        "        sheets[k] = sheets[k].head(MAX_ROWS)\n",
        "total_rows = sum(len(df) for df in sheets.values())\n",
        "print(f\"已讀取工作表數：{len(sheets)}，總筆數：{total_rows}\")\n",
        "\n",
        "\n",
        "def row_to_yaml(row_dict):\n",
        "\n",
        "    try:\n",
        "        import yaml\n",
        "\n",
        "        for k, v in row_dict.items():\n",
        "\n",
        "            if \"Timestamp\" in str(type(v)):\n",
        "                row_dict[k] = str(v)\n",
        "\n",
        "    except Exception:\n",
        "        return \"\\n\".join([f\"{k}: {v}\" for k, v in row_dict.items()])\n",
        "    return yaml.safe_dump(row_dict, allow_unicode=True, sort_keys=False)\n",
        "\n",
        "def build_messages(row_index, row_dict, dynamic_system_prompt):\n",
        "    filtered_dict = {key: value for key, value in row_dict.items() if key in dynamic_system_prompt}\n",
        "\n",
        "    if not filtered_dict:\n",
        "        return None\n",
        "\n",
        "    yaml_text = row_to_yaml(filtered_dict)\n",
        "    user_text = f\"The following is a single data entry（YAML）：\\\\n{yaml_text}\\\\n Please review it according to the guidelines and return valid JSON.\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"{dynamic_system_prompt}\\\\n{user_text}\"}\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "@torch.inference_mode()\n",
        "def validate_one(row_index, row_dict):\n",
        "\n",
        "    messages = build_messages(row_index, row_dict)\n",
        "\n",
        "\n",
        "    prompt = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "    )\n",
        "\n",
        "\n",
        "    inputs = processor(\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    ).to(model.device, dtype=torch_dtype)\n",
        "\n",
        "\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    gen_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512, # max_new_tokens 可從 128 增加到 512\n",
        "        do_sample=False,\n",
        "        temperature=None,\n",
        "        repetition_penalty=1.05,\n",
        "        use_cache=True,\n",
        "    )\n",
        "    text = processor.batch_decode(gen_ids[:, input_len:], skip_special_tokens=True)[0]\n",
        "\n",
        "\n",
        "    jtxt = extract_json_str(text)\n",
        "    parsed = None\n",
        "    is_valid_json = False\n",
        "    try:\n",
        "        parsed = json.loads(jtxt) if jtxt else None\n",
        "        is_valid_json = True if parsed else False\n",
        "    except Exception:\n",
        "        parsed = None\n",
        "        is_valid_json = False\n",
        "\n",
        "    return {\n",
        "        \"raw\": text,\n",
        "        \"json_str\": jtxt,\n",
        "        \"json_obj\": parsed,\n",
        "        \"is_valid_json\": is_valid_json\n",
        "    }\n",
        "\n",
        "\n",
        "def validate_many(start_index, rows, dynamic_system_prompt):\n",
        "    prompts = []\n",
        "    for row_index, row_dict in enumerate(rows):\n",
        "        messages = build_messages(row_index, row_dict, dynamic_system_prompt)\n",
        "        if messages is None:\n",
        "            prompts.append(None)\n",
        "            continue\n",
        "\n",
        "        prompt = processor.apply_chat_template(\n",
        "          messages,\n",
        "          add_generation_prompt=True,\n",
        "          tokenize=False\n",
        "        )\n",
        "        prompts.append(prompt)\n",
        "\n",
        "\n",
        "    valid_prompts = [p for p in prompts if p is not None]\n",
        "    if not valid_prompts:\n",
        "        return [{\"raw\": \"Skipped: No target column found.\", \"summary\": \"Skipped\"} for _ in prompts]\n",
        "\n",
        "\n",
        "    inputs = processor(text=prompts, return_tensors=\"pt\", padding=True)\n",
        "    for k in inputs.keys():\n",
        "        if hasattr(inputs[k], \"to\"):\n",
        "            inputs[k] = inputs[k].to(model.device)\n",
        "\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    gen_ids = model.generate(**inputs, max_new_tokens=512, do_sample=False, repetition_penalty=1.05, use_cache=True)\n",
        "    texts = processor.batch_decode(gen_ids[:, input_len:], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    outs = []\n",
        "\n",
        "    summary_regex = re.compile(r'\"summary\":\\s*\"(.*?)\"', re.DOTALL)\n",
        "    reason_regex = re.compile(r'\"reason\":\\s*\"(.*?)\"', re.DOTALL)\n",
        "\n",
        "    for t in texts:\n",
        "        summary_text = \"\"\n",
        "\n",
        "        summaries_found = summary_regex.findall(t)\n",
        "\n",
        "        if summaries_found:\n",
        "\n",
        "            summary_text = summaries_found[0]\n",
        "        else:\n",
        "\n",
        "            reasons_found = reason_regex.findall(t)\n",
        "            if reasons_found:\n",
        "                summary_text = \"; \".join(reasons_found)\n",
        "            else:\n",
        "                summary_text = f\"Skipped validation due to uncertain words.\"\n",
        "\n",
        "        outs.append({\n",
        "            \"raw\": t,\n",
        "            \"summary\": summary_text\n",
        "        })\n",
        "    return outs\n",
        "\n",
        "\n",
        "def _safe_name(name):\n",
        "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", str(name))\n",
        "\n",
        "BATCH = 64\n",
        "all_records = []\n",
        "per_sheet_records = {}\n",
        "\n",
        "for sheet_name, df in sheets.items():\n",
        "    print(f\"處理工作表：{sheet_name}，筆數：{len(df)}\")\n",
        "\n",
        "\n",
        "    config = PROMPT_CONFIG.get(sheet_name)\n",
        "    if not config:\n",
        "        print(f\"警告：在 PROMPT_CONFIG 中找不到工作表 '{sheet_name}' 的設定，將略過此工作表。\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    dynamic_system_prompt = (\n",
        "        f\"You are a specialized tool for identifying spelling errors in the '{config['target_column']}' field.\",\n",
        "        f\"Your ONLY task is to check for spelling errors in the value of the '{config['target_column']}' field.\",\n",
        "        \"You MUST IGNORE all other fields.\",\n",
        "        f\"If you find a spelling error, your reason MUST be '{config['error_type']}: {config['example_error']}'.\",\n",
        "        \"If there are no spelling errors, your summary must be 'No issues found.'.\",\n",
        "        \"Your entire response MUST be a single, valid JSON object and nothing else.\",\n",
        "\n",
        "        \"\\n--- Example 1: Data with a spelling error ---\\n\"\n",
        "        f\"Example Input Data:\\n\"\n",
        "        f\"{config['target_column']}: {config['example_error'].split(' should be ')[0][1:]}\\n\"\n",
        "        \"Example JSON Output:\\n\"\n",
        "        '{\\n'\n",
        "        '  \"row_index\": 0,\\n'\n",
        "        '  \"issues\": [\\n'\n",
        "        '    {\\n'\n",
        "        f'      \"field\": \"{config[\"target_column\"]}\",\\n'\n",
        "        f'      \"reason\": \"{config[\"error_type\"]}: {config[\"example_error\"]}\",\\n'\n",
        "        '      \"severity\": \"error\"\\n'\n",
        "        '    }\\n'\n",
        "        '  ],\\n'\n",
        "        f'  \"summary\": \"{config[\"error_type\"]}: {config[\"example_error\"]}\"\\n'\n",
        "        '}',\n",
        "\n",
        "        \"\\n--- Example 2: Data with no spelling issues ---\\n\"\n",
        "        f\"Example Input Data:\\n\"\n",
        "        f\"{config['target_column']}: {config['example_correct']}\\n\"\n",
        "        \"Example JSON Output:\\n\"\n",
        "        '{\\n'\n",
        "        '  \"row_index\": 1,\\n'\n",
        "        '  \"issues\": [],\\n'\n",
        "        '  \"summary\": \"No issues found.\"\\n'\n",
        "        '}'\n",
        "    )\n",
        "\n",
        "    dynamic_system_prompt = \"\\n\".join(dynamic_system_prompt)\n",
        "\n",
        "    sheet_recs = []\n",
        "    rows = df.to_dict('records')\n",
        "\n",
        "    for s in range(0, len(rows), BATCH):\n",
        "        batch_rows = rows[s:s+BATCH]\n",
        "        outs = validate_many(s, batch_rows, dynamic_system_prompt)\n",
        "\n",
        "        for j, out in enumerate(outs):\n",
        "            row_idx = s + j\n",
        "            summary = out.get(\"summary\", \"Error: Summary not found.\")\n",
        "            raw_output = out.get(\"raw\", \"Error: Raw output not found.\")\n",
        "\n",
        "\n",
        "\n",
        "            if \"should be\" in summary:\n",
        "                match = re.search(r'\\[(.*?)\\]\\s*should be\\s*\\[(.*?)\\]', summary)\n",
        "                if match:\n",
        "                    original_word = match.group(1)\n",
        "                    corrected_word = match.group(2)\n",
        "\n",
        "                    if original_word.lower() == corrected_word.lower():\n",
        "                        summary = \"No issues found. \"\n",
        "\n",
        "            rec = {\n",
        "                \"sheet_name\": sheet_name,\n",
        "                \"row_index\": int(row_idx),\n",
        "                \"BioMistral_summary\": summary,\n",
        "                #\"raw_model_output\": raw_output\n",
        "            }\n",
        "            sheet_recs.append(rec)\n",
        "            all_records.append(rec)\n",
        "\n",
        "    per_sheet_records[sheet_name] = sheet_recs\n",
        "\n",
        "\n",
        "overview_df = pd.DataFrame(all_records)\n",
        "overview_df.to_csv(\"validation_results__overview.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"已輸出：validation_results__overview.csv\")\n",
        "\n",
        "for sheet_name, df in sheets.items():\n",
        "    result_df = pd.DataFrame(per_sheet_records[sheet_name])\n",
        "\n",
        "    df_with_index = df.reset_index().rename(columns={'index': 'row_index'})\n",
        "\n",
        "    merged = pd.merge(df_with_index, result_df, on=\"row_index\", how=\"left\")\n",
        "\n",
        "    out_name = f\"validation_results__{_safe_name(sheet_name)}.csv\"\n",
        "    merged.to_csv(out_name, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"已輸出：{out_name}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# （選用）微調：SFT（文字-only），預設關閉\n",
        "# ===========================\n",
        "if DO_FINETUNE:\n",
        "    from datasets import Dataset\n",
        "    from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "    from peft import LoraConfig, get_peft_model\n",
        "\n",
        "    valid_records = [r for r in all_records if r.get(\"is_valid_json\", False)]\n",
        "\n",
        "    train_samples = []\n",
        "    for r in valid_records:\n",
        "        row_idx = r[\"row_index\"]\n",
        "        sheet_name = r[\"sheet_name\"]\n",
        "\n",
        "        original_df = sheets.get(sheet_name)\n",
        "        if original_df is None or row_idx >= len(original_df):\n",
        "            continue\n",
        "\n",
        "        row_dict = {k: (None if pd.isna(v) else v) for k, v in original_df.iloc[row_idx].to_dict().items()}\n",
        "        user_yaml = row_to_yaml(row_dict)\n",
        "        user_text = f\"Based on the following data（YAML）：\\n{user_yaml}\\n Please review according to the guidelines and return valid JSON.\"\n",
        "        assistant_text = r[\"medgemma_json\"]\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_text},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_text},\n",
        "        ]\n",
        "        train_samples.append({\"messages\": messages})\n",
        "\n",
        "    if len(train_samples) < 10:\n",
        "        print(f\"可用有效訓練樣本太少 ({len(train_samples)} < 10)，略過微調。請自行提供標註資料或使用能產出更多有效 JSON 的模型，並重試。\")\n",
        "    else:\n",
        "        print(f\"找到 {len(train_samples)} 個有效訓練樣本，開始微調。\")\n",
        "        raw_ds = Dataset.from_list(train_samples)\n",
        "\n",
        "        def encode_fn(ex):\n",
        "            msgs = ex[\"messages\"]\n",
        "            prompt = processor.apply_chat_template(\n",
        "                msgs, add_generation_prompt=False, tokenize=False\n",
        "            )\n",
        "            tokenized = processor(text=prompt, return_tensors=None)\n",
        "            return {\"input_ids\": tokenized[\"input_ids\"], \"labels\": tokenized[\"input_ids\"]}\n",
        "\n",
        "        ds = raw_ds.map(encode_fn, remove_columns=raw_ds.column_names, batched=False)\n",
        "\n",
        "        try:\n",
        "            peft_config = LoraConfig(\n",
        "                r=8, lora_alpha=16, lora_dropout=0.05,\n",
        "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                task_type=\"CAUSAL_LM\",\n",
        "            )\n",
        "            model.enable_input_require_grads()\n",
        "            model = get_peft_model(model, peft_config)\n",
        "        except Exception as e:\n",
        "            print(f\"LoRA 套用失敗，改用全參數微調（小批次）：{e}\")\n",
        "\n",
        "        data_collator = DataCollatorForSeq2Seq(\n",
        "            tokenizer=processor,\n",
        "            model=model,\n",
        "            padding=True,\n",
        "            max_length=4096,\n",
        "            label_pad_token_id=-100,\n",
        "        )\n",
        "\n",
        "        args = TrainingArguments(\n",
        "            output_dir=\"medgemma_sft_out\",\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=8,\n",
        "            learning_rate=2e-5,\n",
        "            num_train_epochs=1,\n",
        "            logging_steps=10,\n",
        "            save_steps=200,\n",
        "            save_total_limit=2,\n",
        "            bf16=torch.cuda.is_available(),\n",
        "            gradient_checkpointing=True,\n",
        "            optim=\"adamw_torch\",\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            warmup_ratio=0.03,\n",
        "            report_to=\"none\",\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=ds,\n",
        "            tokenizer=processor,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "        trainer.train()\n",
        "        trainer.save_model(\"/content/drive/MyDrive/BioMistral_train_backup/BioMistral_sft_out/final\")\n",
        "        processor.save_pretrained(\"/content/drive/MyDrive/BioMistral_train_backup/BioMistral_sft_out/final\")\n",
        "        print(\"微調完成，輸出目錄：/content/drive/MyDrive/BioMistral_train_backup/BioMistral_sft_out/final\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1u0o0p4wchuSF5IX7ClNOK-rb0iTEctS6",
      "authorship_tag": "ABX9TyMC3iBpokDF2KMdfqITGh5L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
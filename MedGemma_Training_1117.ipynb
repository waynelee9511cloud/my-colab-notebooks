{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIuBX6QqhS2O7kqOzuPpQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waynelee9511cloud/my-colab-notebooks/blob/main/MedGemma_Training_1117.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVPZ_YqHQtTl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title MedGemma Training\n",
        "\n",
        "# ==================== CELL 1: å®‰è£å¥—ä»¶ ====================\n",
        "!pip install -q transformers datasets accelerate bitsandbytes peft openpyxl scikit-learn matplotlib seaborn\n",
        "\n",
        "# ==================== CELL 2: å°å…¥å‡½å¼åº« ====================\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import resample\n",
        "from google.colab import files, drive\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print('âœ… æ‰€æœ‰å‡½å¼åº«å°å…¥å®Œæˆï¼')\n",
        "\n",
        "# ==================== CELL 3: æ›è¼‰ Google Drive ====================\n",
        "print('ğŸ“ æ­£åœ¨æ›è¼‰ Google Drive...')\n",
        "drive.mount('/content/drive')\n",
        "print('âœ… Google Drive æ›è¼‰å®Œæˆï¼')\n",
        "\n",
        "# ==================== CELL 4: ä¸Šå‚³æª”æ¡ˆ ====================\n",
        "print('\\nğŸ“¤ è«‹ä¸Šå‚³æ‚¨çš„è¨“ç·´æª”æ¡ˆ')\n",
        "print('  1. Excel è¨“ç·´è³‡æ–™æª”æ¡ˆ')\n",
        "print('  2. å°ç£è—¥ç‰©å•†å“åç¨± txt æª”æ¡ˆï¼ˆé¸å¡«ï¼‰')\n",
        "uploaded = files.upload()\n",
        "\n",
        "excel_file = None\n",
        "drug_names_file = None\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
        "        excel_file = filename\n",
        "        print(f'âœ… Excel æª”æ¡ˆ: {excel_file}')\n",
        "    elif filename.endswith('.txt'):\n",
        "        drug_names_file = filename\n",
        "        print(f'âœ… è—¥ç‰©åç¨±æª”æ¡ˆ: {drug_names_file}')\n",
        "\n",
        "if not excel_file:\n",
        "    raise ValueError('âŒ æœªæ‰¾åˆ° Excel æª”æ¡ˆï¼')\n",
        "\n",
        "# ==================== CELL 5: è®€å–å°ç£è—¥ç‰©åç¨± ====================\n",
        "taiwan_drug_names = set()\n",
        "\n",
        "if drug_names_file:\n",
        "    print(f'\\nğŸ“‹ æ­£åœ¨è®€å–å°ç£è—¥ç‰©å•†å“åç¨±...')\n",
        "    try:\n",
        "        with open(drug_names_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                drug_name = line.strip()\n",
        "                if drug_name:\n",
        "                    taiwan_drug_names.add(drug_name)\n",
        "        print(f'âœ… æˆåŠŸè®€å– {len(taiwan_drug_names)} å€‹å°ç£è—¥ç‰©å•†å“åç¨±')\n",
        "        print(f'   ç¯„ä¾‹: {list(taiwan_drug_names)[:5]}')\n",
        "    except Exception as e:\n",
        "        print(f'âš ï¸ è®€å–è—¥ç‰©åç¨±æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}')\n",
        "else:\n",
        "    print('\\nâš ï¸ æœªæä¾›å°ç£è—¥ç‰©åç¨±æª”æ¡ˆ')\n",
        "\n",
        "# ==================== CELL 6: è®€å– Excel è³‡æ–™ ====================\n",
        "print('\\nğŸ“Š æ­£åœ¨è®€å– Excel æª”æ¡ˆ...')\n",
        "\n",
        "excel_data = pd.ExcelFile(excel_file)\n",
        "sheet_names = excel_data.sheet_names\n",
        "print(f'âœ… ç™¼ç¾ {len(sheet_names)} å€‹å·¥ä½œè¡¨: {sheet_names}')\n",
        "\n",
        "all_dataframes = []\n",
        "\n",
        "for sheet_name in sheet_names:\n",
        "    df_sheet = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
        "\n",
        "    required_columns = ['Term_To_Check', 'Correct_Output', 'category']\n",
        "    missing_columns = [col for col in required_columns if col not in df_sheet.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f'âš ï¸ å·¥ä½œè¡¨ \"{sheet_name}\" ç¼ºå°‘æ¬„ä½: {missing_columns}')\n",
        "        continue\n",
        "\n",
        "    total = len(df_sheet)\n",
        "    correct = len(df_sheet[df_sheet['Correct_Output'] == 'No issues found.'])\n",
        "    error = len(df_sheet[df_sheet['Correct_Output'] != 'No issues found.'])\n",
        "\n",
        "    print(f'  ğŸ“„ {sheet_name}: {total} ç­† (éŒ¯èª¤: {error} / {error/total*100:.1f}%)')\n",
        "    all_dataframes.append(df_sheet)\n",
        "\n",
        "df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "print(f'\\nâœ… è³‡æ–™åˆä½µå®Œæˆ')\n",
        "print(f'  ç¸½ç­†æ•¸: {len(df)}')\n",
        "print(f'  æ­£ç¢ºè¡“èª: {len(df[df[\"Correct_Output\"] == \"No issues found.\"])} ç­†')\n",
        "print(f'  éŒ¯èª¤è¡“èª: {len(df[df[\"Correct_Output\"] != \"No issues found.\"])} ç­†')\n",
        "\n",
        "# ==================== CELL 7: æ•´åˆå°ç£è—¥ç‰©åç¨± ====================\n",
        "if taiwan_drug_names and len(taiwan_drug_names) > 0:\n",
        "    print('\\n' + '=' * 80)\n",
        "    print('ğŸ‡¹ğŸ‡¼ æ•´åˆå°ç£è—¥ç‰©å•†å“åç¨±åˆ°è¨“ç·´è³‡æ–™')\n",
        "    print('=' * 80)\n",
        "\n",
        "    drug_training_data = []\n",
        "    for drug_name in taiwan_drug_names:\n",
        "        drug_training_data.append({\n",
        "            'Term_To_Check': drug_name,\n",
        "            'Correct_Output': 'No issues found.',\n",
        "            'category': 'CM'\n",
        "        })\n",
        "\n",
        "    df_drugs = pd.DataFrame(drug_training_data)\n",
        "    original_len = len(df)\n",
        "    df = pd.concat([df, df_drugs], ignore_index=True)\n",
        "\n",
        "    print(f'\\nâœ… æˆåŠŸæ•´åˆ {len(df_drugs)} å€‹å°ç£è—¥ç‰©åç¨±')\n",
        "    print(f'   åŸå§‹è³‡æ–™: {original_len} ç­†')\n",
        "    print(f'   æ•´åˆå¾Œ: {len(df)} ç­†')\n",
        "    print('=' * 80)\n",
        "\n",
        "# ==================== CELL 8: è³‡æ–™å¹³è¡¡ ====================\n",
        "print('\\n' + '=' * 80)\n",
        "print('ğŸ“Š è³‡æ–™å¹³è¡¡è™•ç†')\n",
        "print('=' * 80)\n",
        "\n",
        "TARGET_ERROR_RATIO = 0.40\n",
        "MAX_TOTAL_SAMPLES = 3500\n",
        "\n",
        "current_error_count = len(df[df['Correct_Output'] != 'No issues found.'])\n",
        "current_correct_count = len(df[df['Correct_Output'] == 'No issues found.'])\n",
        "current_ratio = current_error_count / len(df)\n",
        "\n",
        "print(f'\\nğŸ“ˆ è³‡æ–™åˆ†å¸ƒ:')\n",
        "print(f'  ç¸½ç­†æ•¸: {len(df)}')\n",
        "print(f'  æ­£ç¢ºè¡“èª: {current_correct_count} ({current_correct_count/len(df)*100:.1f}%)')\n",
        "print(f'  éŒ¯èª¤è¡“èª: {current_error_count} ({current_ratio*100:.1f}%)')\n",
        "\n",
        "if current_ratio < TARGET_ERROR_RATIO:\n",
        "    print(f'\\nâš ï¸ éŒ¯èª¤æ¯”ä¾‹ä¸è¶³ï¼Œé–‹å§‹éæ¡æ¨£...')\n",
        "\n",
        "    df_correct = df[df['Correct_Output'] == 'No issues found.'].copy()\n",
        "    df_error = df[df['Correct_Output'] != 'No issues found.'].copy()\n",
        "\n",
        "    needed_error_count = int(TARGET_ERROR_RATIO * len(df_correct) / (1 - TARGET_ERROR_RATIO))\n",
        "    potential_total = len(df_correct) + needed_error_count\n",
        "\n",
        "    if potential_total > MAX_TOTAL_SAMPLES:\n",
        "        target_error_count = int(MAX_TOTAL_SAMPLES * TARGET_ERROR_RATIO)\n",
        "        target_correct_count = MAX_TOTAL_SAMPLES - target_error_count\n",
        "\n",
        "        if len(df_correct) > target_correct_count:\n",
        "            df_correct = resample(df_correct, replace=False, n_samples=target_correct_count, random_state=42)\n",
        "\n",
        "        df_error_upsampled = resample(df_error, replace=True, n_samples=target_error_count, random_state=42)\n",
        "    else:\n",
        "        df_error_upsampled = resample(df_error, replace=True, n_samples=needed_error_count, random_state=42)\n",
        "\n",
        "    df = pd.concat([df_correct, df_error_upsampled], ignore_index=True)\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f'\\nğŸ‰ è³‡æ–™å¹³è¡¡å®Œæˆï¼')\n",
        "    print(f'  ç¸½ç­†æ•¸: {len(df)}')\n",
        "\n",
        "    del df_correct, df_error, df_error_upsampled\n",
        "    gc.collect()\n",
        "\n",
        "print('=' * 80)\n",
        "\n",
        "# ==================== CELL 9: è¨­å®šåƒæ•¸ ====================\n",
        "BASE_MODEL_ID = 'google/medgemma-4b-it'\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 2\n",
        "MAX_LENGTH = 384\n",
        "GRADIENT_ACCUMULATION_STEPS = 8\n",
        "N_SPLITS = 5\n",
        "\n",
        "print(f'\\nâš™ï¸ è¨“ç·´åƒæ•¸è¨­å®š')\n",
        "print('=' * 50)\n",
        "print(f'ä½¿ç”¨æ¨¡å‹: {BASE_MODEL_ID}')\n",
        "print(f'äº¤å‰é©—è­‰æŠ˜æ•¸: {N_SPLITS}-fold')\n",
        "print(f'å­¸ç¿’ç‡: {LEARNING_RATE}')\n",
        "print(f'è¨“ç·´è¼ªæ•¸: {NUM_EPOCHS}')\n",
        "print(f'æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}')\n",
        "print('=' * 50)\n",
        "\n",
        "# ==================== CELL 10: æº–å‚™è¨“ç·´è³‡æ–™æ ¼å¼ ====================\n",
        "def create_prompt(term):\n",
        "    return f'Please check if there is any error in this medical term: {term}'\n",
        "\n",
        "def format_example(term, response, category):\n",
        "    prompt = create_prompt(term)\n",
        "    formatted_text = f'<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn>'\n",
        "    return {\n",
        "        'text': formatted_text,\n",
        "        'term': term,\n",
        "        'response': response,\n",
        "        'category': category\n",
        "    }\n",
        "\n",
        "all_examples = []\n",
        "for idx, row in df.iterrows():\n",
        "    example = format_example(\n",
        "        term=row['Term_To_Check'],\n",
        "        response=row['Correct_Output'],\n",
        "        category=row['category']\n",
        "    )\n",
        "    all_examples.append(example)\n",
        "\n",
        "print(f'âœ… æº–å‚™äº† {len(all_examples)} å€‹è¨“ç·´ç¯„ä¾‹')\n",
        "\n",
        "# ==================== CELL 11: åˆå§‹åŒ–äº¤å‰é©—è­‰ ====================\n",
        "print('\\n' + '=' * 80)\n",
        "print('ğŸ”„ é–‹å§‹ 5-Fold äº¤å‰é©—è­‰')\n",
        "print('=' * 80)\n",
        "\n",
        "fold_results = {\n",
        "    'accuracy': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'f1': [],\n",
        "    'confusion_matrices': [],\n",
        "    'category_results': []\n",
        "}\n",
        "\n",
        "kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
        "data_indices = list(range(len(all_examples)))\n",
        "\n",
        "print(f'âœ… åˆå§‹åŒ–å®Œæˆ')\n",
        "\n",
        "# ==================== æ¥ä¸‹ä¾†çš„ CELL 12-16: æ¯å€‹ Fold çš„è¨“ç·´ ====================\n",
        "# æ³¨æ„ï¼šç”±æ–¼æ¯å€‹ fold çš„è¨“ç·´ç¨‹å¼ç¢¼å¾ˆé•·ï¼Œå»ºè­°å°‡ä¸‹é¢çš„ç¨‹å¼ç¢¼åˆ†æˆ 5 å€‹ç¨ç«‹çš„ cell\n",
        "# æ¯å€‹ cell è¨“ç·´ä¸€å€‹ foldï¼Œé€™æ¨£å¯ä»¥åœ¨ä»»ä½•ä¸€å€‹ fold å¤±æ•—æ™‚ä¸å½±éŸ¿å…¶ä»– fold\n",
        "\n",
        "# ä»¥ä¸‹æ˜¯é€šç”¨çš„è¨“ç·´å¾ªç’°ç¨‹å¼ç¢¼ï¼Œæ‚¨å¯ä»¥è¤‡è£½ 5 æ¬¡ä¸¦ä¿®æ”¹ fold_idx\n",
        "\n",
        "# ==================== CELL 12-16: Fold è¨“ç·´å¾ªç’° (è«‹åˆ†æˆ5å€‹cellåŸ·è¡Œ) ====================\n",
        "# å°‡ä»¥ä¸‹ç¨‹å¼ç¢¼è¤‡è£½5æ¬¡ï¼Œæ¯æ¬¡ä¿®æ”¹ fold_idx çš„å€¼(0, 1, 2, 3, 4)\n",
        "\n",
        "for fold_idx, (train_indices, test_indices) in enumerate(kfold.split(data_indices)):\n",
        "    print(f'\\n' + '=' * 80)\n",
        "    print(f'ğŸ“Š Fold {fold_idx + 1}/{N_SPLITS}')\n",
        "    print('=' * 80)\n",
        "\n",
        "    # åˆ†å‰²è³‡æ–™\n",
        "    train_examples = [all_examples[i] for i in train_indices]\n",
        "    test_examples = [all_examples[i] for i in test_indices]\n",
        "\n",
        "    print(f'  è¨“ç·´é›†å¤§å°: {len(train_examples)}')\n",
        "    print(f'  æ¸¬è©¦é›†å¤§å°: {len(test_examples)}')\n",
        "\n",
        "    # è¼‰å…¥æ¨¡å‹\n",
        "    print(f'\\nğŸ”§ Fold {fold_idx + 1}: è¼‰å…¥æ¨¡å‹...')\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = 'right'\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map='auto',\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print('âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ')\n",
        "\n",
        "    # é…ç½® LoRA\n",
        "    print(f'\\nğŸ”§ Fold {fold_idx + 1}: é…ç½® LoRA...')\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.1,\n",
        "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "        task_type='CAUSAL_LM',\n",
        "        bias='none'\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    print('âœ… LoRA é…ç½®å®Œæˆ')\n",
        "\n",
        "    # æº–å‚™è¨“ç·´è³‡æ–™\n",
        "    print(f'\\nğŸ”„ Fold {fold_idx + 1}: æº–å‚™è¨“ç·´è³‡æ–™...')\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        texts = examples['text'] if isinstance(examples['text'], list) else [examples['text']]\n",
        "        tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=None)\n",
        "        tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "        return tokenized\n",
        "\n",
        "    train_dataset_dict = {\n",
        "        'text': [ex['text'] for ex in train_examples],\n",
        "        'term': [ex['term'] for ex in train_examples],\n",
        "        'response': [ex['response'] for ex in train_examples],\n",
        "        'category': [ex['category'] for ex in train_examples]\n",
        "    }\n",
        "\n",
        "    test_dataset_dict = {\n",
        "        'text': [ex['text'] for ex in test_examples],\n",
        "        'term': [ex['term'] for ex in test_examples],\n",
        "        'response': [ex['response'] for ex in test_examples],\n",
        "        'category': [ex['category'] for ex in test_examples]\n",
        "    }\n",
        "\n",
        "    train_dataset = Dataset.from_dict(train_dataset_dict)\n",
        "    test_dataset = Dataset.from_dict(test_dataset_dict)\n",
        "\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\n",
        "\n",
        "    print('âœ… è³‡æ–™æº–å‚™å®Œæˆ')\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # è¨­å®šè¨“ç·´åƒæ•¸\n",
        "    output_dir = f'/content/drive/MyDrive/medgemma_cv_fold_{fold_idx + 1}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        lr_scheduler_type='cosine',\n",
        "        warmup_ratio=0.1,\n",
        "        eval_strategy='steps',\n",
        "        eval_steps=100,\n",
        "        save_strategy='steps',\n",
        "        save_steps=100,\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='eval_loss',\n",
        "        greater_is_better=False,\n",
        "        logging_steps=20,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        push_to_hub=False,\n",
        "        report_to='none',\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False,\n",
        "        seed=42 + fold_idx,\n",
        "        data_seed=42 + fold_idx,\n",
        "        optim='paged_adamw_8bit',\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0,\n",
        "        gradient_checkpointing=True,\n",
        "        dataloader_num_workers=0,\n",
        "    )\n",
        "\n",
        "    # è¨“ç·´æ¨¡å‹\n",
        "    print(f'\\nğŸš€ Fold {fold_idx + 1}: é–‹å§‹è¨“ç·´...')\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        train_result = trainer.train()\n",
        "        print(f'\\nâœ… Fold {fold_idx + 1} è¨“ç·´å®Œæˆ')\n",
        "        trainer.save_model(f'{output_dir}/final_model')\n",
        "        tokenizer.save_pretrained(f'{output_dir}/final_model')\n",
        "    except RuntimeError as e:\n",
        "        if 'out of memory' in str(e):\n",
        "            print(f'\\nâŒ Fold {fold_idx + 1}: Out of Memory éŒ¯èª¤')\n",
        "        continue\n",
        "\n",
        "    # è©•ä¼°æ¨¡å‹\n",
        "    print(f'\\nğŸ“Š Fold {fold_idx + 1}: è©•ä¼°æ¸¬è©¦é›†...')\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "    categories = []\n",
        "\n",
        "    for i, example in enumerate(test_examples):\n",
        "        if i % 50 == 0:\n",
        "            print(f'  é€²åº¦: {i}/{len(test_examples)}')\n",
        "\n",
        "        term = example['term']\n",
        "        true_response = example['response']\n",
        "        category = example['category']\n",
        "\n",
        "        prompt = create_prompt(term)\n",
        "        formatted = f'<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n'\n",
        "\n",
        "        inputs = tokenizer(formatted, return_tensors='pt').to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=100,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                do_sample=False,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        pred_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        pred_label = 1 if ('incorrect' in pred_text.lower() or 'error' in pred_text.lower()) else 0\n",
        "        true_label = 0 if true_response == 'No issues found.' else 1\n",
        "\n",
        "        predictions.append(pred_label)\n",
        "        ground_truths.append(true_label)\n",
        "        categories.append(category)\n",
        "\n",
        "    # è¨ˆç®—æŒ‡æ¨™\n",
        "    accuracy = accuracy_score(ground_truths, predictions)\n",
        "    precision = precision_score(ground_truths, predictions, zero_division=0)\n",
        "    recall = recall_score(ground_truths, predictions, zero_division=0)\n",
        "    f1 = f1_score(ground_truths, predictions, zero_division=0)\n",
        "    cm = confusion_matrix(ground_truths, predictions)\n",
        "\n",
        "    fold_results['accuracy'].append(accuracy)\n",
        "    fold_results['precision'].append(precision)\n",
        "    fold_results['recall'].append(recall)\n",
        "    fold_results['f1'].append(f1)\n",
        "    fold_results['confusion_matrices'].append(cm)\n",
        "\n",
        "    print(f'\\nğŸ“ˆ Fold {fold_idx + 1} è©•ä¼°çµæœ:')\n",
        "    print(f'  Accuracy:  {accuracy:.4f}')\n",
        "    print(f'  Precision: {precision:.4f}')\n",
        "    print(f'  Recall:    {recall:.4f}')\n",
        "    print(f'  F1 Score:  {f1:.4f}')\n",
        "\n",
        "    # å„é¡åˆ¥æ€§èƒ½åˆ†æ\n",
        "    category_metrics = {}\n",
        "    unique_categories = set(categories)\n",
        "\n",
        "    for cat in unique_categories:\n",
        "        cat_indices = [i for i, c in enumerate(categories) if c == cat]\n",
        "        cat_predictions = [predictions[i] for i in cat_indices]\n",
        "        cat_ground_truths = [ground_truths[i] for i in cat_indices]\n",
        "\n",
        "        if len(cat_indices) > 0:\n",
        "            cat_accuracy = accuracy_score(cat_ground_truths, cat_predictions)\n",
        "            cat_precision = precision_score(cat_ground_truths, cat_predictions, zero_division=0)\n",
        "            cat_recall = recall_score(cat_ground_truths, cat_predictions, zero_division=0)\n",
        "            cat_f1 = f1_score(cat_ground_truths, cat_predictions, zero_division=0)\n",
        "\n",
        "            category_metrics[cat] = {\n",
        "                'accuracy': cat_accuracy,\n",
        "                'precision': cat_precision,\n",
        "                'recall': cat_recall,\n",
        "                'f1': cat_f1,\n",
        "                'samples': len(cat_indices)\n",
        "            }\n",
        "\n",
        "    fold_results['category_results'].append(category_metrics)\n",
        "\n",
        "    # æ¸…ç†è¨˜æ†¶é«”\n",
        "    del model, trainer, train_dataset, test_dataset\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f'\\nâœ… Fold {fold_idx + 1} å®Œæˆï¼')\n",
        "    print('=' * 80)\n",
        "\n",
        "# ==================== CELL 17: çµ±è¨ˆåˆ†æ ====================\n",
        "print('\\n' + '=' * 80)\n",
        "print('ğŸ“Š äº¤å‰é©—è­‰çµ±è¨ˆåˆ†æ')\n",
        "print('=' * 80)\n",
        "\n",
        "metrics_stats = {}\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    values = fold_results[metric]\n",
        "    mean = np.mean(values)\n",
        "    std = np.std(values, ddof=1)\n",
        "\n",
        "    confidence_level = 0.95\n",
        "    degrees_freedom = len(values) - 1\n",
        "    confidence_interval = stats.t.interval(confidence_level, degrees_freedom, mean, stats.sem(values))\n",
        "\n",
        "    metrics_stats[metric] = {\n",
        "        'mean': mean,\n",
        "        'std': std,\n",
        "        'ci_lower': confidence_interval[0],\n",
        "        'ci_upper': confidence_interval[1],\n",
        "        'values': values\n",
        "    }\n",
        "\n",
        "    print(f'\\n{metric.upper()}:')\n",
        "    print(f'  å„ Fold çµæœ: {[f\"{v:.4f}\" for v in values]}')\n",
        "    print(f'  å¹³å‡å€¼: {mean:.4f}')\n",
        "    print(f'  æ¨™æº–å·®: {std:.4f}')\n",
        "    print(f'  95% ä¿¡è³´å€é–“: [{confidence_interval[0]:.4f}, {confidence_interval[1]:.4f}]')\n",
        "\n",
        "# ==================== CELL 18: æ··æ·†çŸ©é™£è¦–è¦ºåŒ– ====================\n",
        "avg_cm = np.mean(fold_results['confusion_matrices'], axis=0)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('5-Fold Cross Validation - Confusion Matrices', fontsize=16, fontweight='bold')\n",
        "\n",
        "for idx in range(N_SPLITS):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    cm = fold_results['confusion_matrices'][idx]\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=['Correct', 'Error'],\n",
        "                yticklabels=['Correct', 'Error'])\n",
        "    ax.set_title(f'Fold {idx + 1}\\nF1={fold_results[\"f1\"][idx]:.4f}')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "\n",
        "ax = axes[1, 2]\n",
        "sns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Greens', ax=ax,\n",
        "            xticklabels=['Correct', 'Error'],\n",
        "            yticklabels=['Correct', 'Error'])\n",
        "ax.set_title(f'Average\\nF1={metrics_stats[\"f1\"][\"mean\"]:.4f}Â±{metrics_stats[\"f1\"][\"std\"]:.4f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "output_path = '/content/drive/MyDrive/confusion_matrices_cv.png'\n",
        "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "print(f'\\nâœ… æ··æ·†çŸ©é™£å·²ä¿å­˜è‡³: {output_path}')\n",
        "plt.show()\n",
        "\n",
        "# ==================== CELL 19: æ€§èƒ½æŒ‡æ¨™è¦–è¦ºåŒ– ====================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('5-Fold Cross Validation - Performance Metrics', fontsize=16, fontweight='bold')\n",
        "\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
        "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "\n",
        "for idx, (metric, name) in enumerate(zip(metrics_to_plot, metric_names)):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    values = metrics_stats[metric]['values']\n",
        "    mean = metrics_stats[metric]['mean']\n",
        "    ci_lower = metrics_stats[metric]['ci_lower']\n",
        "    ci_upper = metrics_stats[metric]['ci_upper']\n",
        "\n",
        "    folds = list(range(1, N_SPLITS + 1))\n",
        "    ax.plot(folds, values, marker='o', linewidth=2, markersize=8, label='Fold Results')\n",
        "    ax.axhline(y=mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:.4f}')\n",
        "    ax.axhspan(ci_lower, ci_upper, alpha=0.2, color='red', label=f'95% CI')\n",
        "\n",
        "    ax.set_xlabel('Fold', fontsize=12)\n",
        "    ax.set_ylabel(name, fontsize=12)\n",
        "    ax.set_title(f'{name}\\nMean: {mean:.4f} Â± {metrics_stats[metric][\"std\"]:.4f}', fontsize=12)\n",
        "    ax.set_xticks(folds)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    ax.set_ylim([min(0.9, min(values) - 0.02), 1.0])\n",
        "\n",
        "plt.tight_layout()\n",
        "output_path = '/content/drive/MyDrive/performance_metrics_cv.png'\n",
        "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "print(f'\\nâœ… æ€§èƒ½æŒ‡æ¨™åœ–å·²ä¿å­˜è‡³: {output_path}')\n",
        "plt.show()\n",
        "\n",
        "# ==================== CELL 20: ç”Ÿæˆè©³ç´°å ±å‘Š ====================\n",
        "report_lines = []\n",
        "report_lines.append('=' * 80)\n",
        "report_lines.append('MedGemma é†«ç™‚è¡“èªæ ¡æ­£æ¨¡å‹ - 5-Fold äº¤å‰é©—è­‰å ±å‘Š')\n",
        "report_lines.append('=' * 80)\n",
        "report_lines.append('')\n",
        "\n",
        "report_lines.append('ã€åŸºæœ¬è³‡è¨Šã€‘')\n",
        "report_lines.append(f'  æ¨¡å‹: {BASE_MODEL_ID}')\n",
        "report_lines.append(f'  è¨“ç·´è³‡æ–™ç¸½æ•¸: {len(all_examples)}')\n",
        "report_lines.append(f'  äº¤å‰é©—è­‰æŠ˜æ•¸: {N_SPLITS}')\n",
        "if taiwan_drug_names:\n",
        "    report_lines.append(f'  å°ç£è—¥ç‰©åç¨±æ•¸é‡: {len(taiwan_drug_names)}')\n",
        "report_lines.append('')\n",
        "\n",
        "report_lines.append('ã€æ•´é«”æ€§èƒ½çµ±è¨ˆã€‘')\n",
        "for metric, name in zip(['accuracy', 'precision', 'recall', 'f1'],\n",
        "                       ['Accuracy', 'Precision', 'Recall', 'F1 Score']):\n",
        "    stats_data = metrics_stats[metric]\n",
        "    report_lines.append(f'\\n{name}:')\n",
        "    report_lines.append(f'  å¹³å‡å€¼: {stats_data[\"mean\"]:.4f}')\n",
        "    report_lines.append(f'  æ¨™æº–å·®: {stats_data[\"std\"]:.4f}')\n",
        "    report_lines.append(f'  95% ä¿¡è³´å€é–“: [{stats_data[\"ci_lower\"]:.4f}, {stats_data[\"ci_upper\"]:.4f}]')\n",
        "\n",
        "report_lines.append('\\n' + '=' * 80)\n",
        "\n",
        "report_text = '\\n'.join(report_lines)\n",
        "report_path = '/content/drive/MyDrive/cv_report.txt'\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(report_text)\n",
        "\n",
        "print(f'\\nâœ… è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_path}')\n",
        "print('\\nå ±å‘Šå…§å®¹:')\n",
        "print(report_text)\n",
        "\n",
        "print('\\n' + '=' * 80)\n",
        "print('ğŸ‰ 5-Fold äº¤å‰é©—è­‰å®Œæˆï¼')\n",
        "print('=' * 80)\n",
        "print(f'\\nğŸ“Š æœ€çµ‚çµæœæ‘˜è¦:')\n",
        "print(f'  F1 Score:  {metrics_stats[\"f1\"][\"mean\"]:.4f} Â± {metrics_stats[\"f1\"][\"std\"]:.4f}')\n",
        "print(f'  Accuracy:  {metrics_stats[\"accuracy\"][\"mean\"]:.4f} Â± {metrics_stats[\"accuracy\"][\"std\"]:.4f}')"
      ]
    }
  ]
}
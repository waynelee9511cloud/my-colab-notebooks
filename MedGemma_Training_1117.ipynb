{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIuBX6QqhS2O7kqOzuPpQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waynelee9511cloud/my-colab-notebooks/blob/main/MedGemma_Training_1117.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVPZ_YqHQtTl",
        "cellView": "form"
      },
      "outputs": [],
      "source": "# ==================== CELL 1: å®‰è£å¥—ä»¶ ====================\nprint('ğŸ“¦ é–‹å§‹å®‰è£å¿…è¦å¥—ä»¶...')\n!pip install -q transformers datasets accelerate bitsandbytes peft openpyxl scikit-learn matplotlib seaborn\nprint('âœ… å¥—ä»¶å®‰è£å®Œæˆï¼')"
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 2: å°å…¥å‡½å¼åº« ====================\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils import resample\nfrom google.colab import files, drive\nimport os\nimport json\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('âœ… æ‰€æœ‰å‡½å¼åº«å°å…¥å®Œæˆï¼')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 3: GPU æª¢æŸ¥ ====================\nprint('ğŸ” æª¢æŸ¥ GPU ç‹€æ…‹...')\nif not torch.cuda.is_available():\n    raise RuntimeError('âŒ éŒ¯èª¤ï¼šéœ€è¦ GPU æ‰èƒ½åŸ·è¡Œæ­¤ç¨‹å¼ï¼è«‹ç¢ºä¿å·²å•Ÿç”¨ GPUã€‚')\n\ngpu_name = torch.cuda.get_device_name(0)\ngpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n\nprint(f'âœ… GPU å·²å°±ç·’ï¼')\nprint(f'  GPU å‹è™Ÿ: {gpu_name}')\nprint(f'  GPU è¨˜æ†¶é«”: {gpu_memory:.2f} GB')\n\n# æ¸…ç©º GPU å¿«å–\ntorch.cuda.empty_cache()\ngc.collect()\nprint('âœ… GPU å¿«å–å·²æ¸…ç©º')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 4: æ›è¼‰ Google Drive ====================\nprint('ğŸ“ æ­£åœ¨æ›è¼‰ Google Drive...')\ndrive.mount('/content/drive')\n\n# æª¢æŸ¥ä¸¦å‰µå»ºè¼¸å‡ºç›®éŒ„\noutput_base_dir = '/content/drive/MyDrive/MedGemma_Training_Output'\nos.makedirs(output_base_dir, exist_ok=True)\n\nprint('âœ… Google Drive æ›è¼‰å®Œæˆï¼')\nprint(f'  è¼¸å‡ºç›®éŒ„: {output_base_dir}')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 5: ä¸Šå‚³æª”æ¡ˆ ====================\nprint('\\nğŸ“¤ è«‹ä¸Šå‚³æ‚¨çš„è¨“ç·´æª”æ¡ˆ')\nprint('  1. Excel è¨“ç·´è³‡æ–™æª”æ¡ˆ (å¿…é ˆ)')\nprint('  2. å°ç£è—¥ç‰©å•†å“åç¨± txt æª”æ¡ˆï¼ˆé¸å¡«ï¼‰')\nprint()\n\nuploaded = files.upload()\n\nexcel_file = None\ndrug_names_file = None\n\nfor filename in uploaded.keys():\n    if filename.endswith('.xlsx') or filename.endswith('.xls'):\n        excel_file = filename\n        print(f'âœ… Excel æª”æ¡ˆ: {excel_file}')\n    elif filename.endswith('.txt'):\n        drug_names_file = filename\n        print(f'âœ… è—¥ç‰©åç¨±æª”æ¡ˆ: {drug_names_file}')\n\nif not excel_file:\n    raise ValueError('âŒ æœªæ‰¾åˆ° Excel æª”æ¡ˆï¼è«‹ä¸Šå‚³ .xlsx æˆ– .xls æ ¼å¼çš„æª”æ¡ˆã€‚')\n\nprint('\\nâœ… æª”æ¡ˆä¸Šå‚³å®Œæˆï¼')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 6: è®€å–å°ç£è—¥ç‰©åç¨± ====================\ntaiwan_drug_names = set()\n\nif drug_names_file:\n    print(f'\\nğŸ“‹ æ­£åœ¨è®€å–å°ç£è—¥ç‰©å•†å“åç¨±...')\n    try:\n        with open(drug_names_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                drug_name = line.strip()\n                if drug_name:\n                    taiwan_drug_names.add(drug_name)\n        print(f'âœ… æˆåŠŸè®€å– {len(taiwan_drug_names)} å€‹å°ç£è—¥ç‰©å•†å“åç¨±')\n        print(f'   ç¯„ä¾‹: {list(taiwan_drug_names)[:5]}')\n    except Exception as e:\n        print(f'âš ï¸ è®€å–è—¥ç‰©åç¨±æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}')\n        print(f'   å°‡ç¹¼çºŒåŸ·è¡Œï¼Œä½†ä¸åŒ…å«è—¥ç‰©åç¨±è³‡æ–™')\nelse:\n    print('\\nâš ï¸ æœªæä¾›å°ç£è—¥ç‰©åç¨±æª”æ¡ˆï¼Œå°‡åƒ…ä½¿ç”¨ Excel è³‡æ–™è¨“ç·´')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 7: è®€å– Excel è³‡æ–™ ====================\nprint('\\nğŸ“Š æ­£åœ¨è®€å– Excel æª”æ¡ˆ...')\n\nexcel_data = pd.ExcelFile(excel_file)\nsheet_names = excel_data.sheet_names\nprint(f'âœ… ç™¼ç¾ {len(sheet_names)} å€‹å·¥ä½œè¡¨: {sheet_names}')\n\nall_dataframes = []\n\nfor sheet_name in sheet_names:\n    df_sheet = pd.read_excel(excel_data, sheet_name=sheet_name)\n    \n    required_columns = ['Term_To_Check', 'Correct_Output', 'category']\n    missing_columns = [col for col in required_columns if col not in df_sheet.columns]\n    \n    if missing_columns:\n        print(f'âš ï¸ å·¥ä½œè¡¨ \"{sheet_name}\" ç¼ºå°‘æ¬„ä½: {missing_columns}ï¼Œè·³éæ­¤å·¥ä½œè¡¨')\n        continue\n    \n    total = len(df_sheet)\n    correct = len(df_sheet[df_sheet['Correct_Output'] == 'No issues found.'])\n    error = len(df_sheet[df_sheet['Correct_Output'] != 'No issues found.'])\n    \n    print(f'  ğŸ“„ {sheet_name}: {total} ç­† (æ­£ç¢º: {correct}, éŒ¯èª¤: {error} / {error/total*100:.1f}%)')\n    all_dataframes.append(df_sheet)\n\nif not all_dataframes:\n    raise ValueError('âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å·¥ä½œè¡¨ï¼è«‹ç¢ºä¿ Excel æª”æ¡ˆåŒ…å«å¿…è¦çš„æ¬„ä½ã€‚')\n\ndf = pd.concat(all_dataframes, ignore_index=True)\n\nprint(f'\\nâœ… è³‡æ–™åˆä½µå®Œæˆ')\nprint(f'  ç¸½ç­†æ•¸: {len(df)}')\nprint(f'  æ­£ç¢ºè¡“èª: {len(df[df[\"Correct_Output\"] == \"No issues found.\"])} ç­†')\nprint(f'  éŒ¯èª¤è¡“èª: {len(df[df[\"Correct_Output\"] != \"No issues found.\"])} ç­†')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 8: æ•´åˆå°ç£è—¥ç‰©åç¨± ====================\nif taiwan_drug_names and len(taiwan_drug_names) > 0:\n    print('\\n' + '=' * 80)\n    print('ğŸ‡¹ğŸ‡¼ æ•´åˆå°ç£è—¥ç‰©å•†å“åç¨±åˆ°è¨“ç·´è³‡æ–™')\n    print('=' * 80)\n    \n    drug_training_data = []\n    for drug_name in taiwan_drug_names:\n        drug_training_data.append({\n            'Term_To_Check': drug_name,\n            'Correct_Output': 'No issues found.',\n            'category': 'CM'\n        })\n    \n    df_drugs = pd.DataFrame(drug_training_data)\n    original_len = len(df)\n    df = pd.concat([df, df_drugs], ignore_index=True)\n    \n    print(f'\\nâœ… æˆåŠŸæ•´åˆ {len(df_drugs)} å€‹å°ç£è—¥ç‰©åç¨±')\n    print(f'   åŸå§‹è³‡æ–™: {original_len} ç­†')\n    print(f'   æ•´åˆå¾Œ: {len(df)} ç­†')\n    print('=' * 80)\nelse:\n    print('\\nâ­ï¸ è·³éè—¥ç‰©åç¨±æ•´åˆæ­¥é©Ÿ')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 9: è³‡æ–™å¹³è¡¡ ====================\nprint('\\n' + '=' * 80)\nprint('ğŸ“Š è³‡æ–™å¹³è¡¡è™•ç†')\nprint('=' * 80)\n\nTARGET_ERROR_RATIO = 0.40\nMAX_TOTAL_SAMPLES = 3500\n\ncurrent_error_count = len(df[df['Correct_Output'] != 'No issues found.'])\ncurrent_correct_count = len(df[df['Correct_Output'] == 'No issues found.'])\ncurrent_ratio = current_error_count / len(df)\n\nprint(f'\\nğŸ“ˆ ç•¶å‰è³‡æ–™åˆ†å¸ƒ:')\nprint(f'  ç¸½ç­†æ•¸: {len(df)}')\nprint(f'  æ­£ç¢ºè¡“èª: {current_correct_count} ({current_correct_count/len(df)*100:.1f}%)')\nprint(f'  éŒ¯èª¤è¡“èª: {current_error_count} ({current_ratio*100:.1f}%)')\nprint(f'  ç›®æ¨™éŒ¯èª¤æ¯”ä¾‹: {TARGET_ERROR_RATIO*100:.1f}%')\n\nif current_ratio < TARGET_ERROR_RATIO:\n    print(f'\\nâš™ï¸ éŒ¯èª¤æ¯”ä¾‹ ({current_ratio*100:.1f}%) ä½æ–¼ç›®æ¨™ ({TARGET_ERROR_RATIO*100:.1f}%)ï¼Œé–‹å§‹éæ¡æ¨£...')\n    \n    df_correct = df[df['Correct_Output'] == 'No issues found.'].copy()\n    df_error = df[df['Correct_Output'] != 'No issues found.'].copy()\n    \n    # è¨ˆç®—éœ€è¦çš„éŒ¯èª¤æ¨£æœ¬æ•¸\n    needed_error_count = int(TARGET_ERROR_RATIO * len(df_correct) / (1 - TARGET_ERROR_RATIO))\n    potential_total = len(df_correct) + needed_error_count\n    \n    if potential_total > MAX_TOTAL_SAMPLES:\n        print(f'  âš ï¸ é è¨ˆç¸½æ•¸ ({potential_total}) è¶…éä¸Šé™ ({MAX_TOTAL_SAMPLES})ï¼Œé€²è¡Œèª¿æ•´...')\n        target_error_count = int(MAX_TOTAL_SAMPLES * TARGET_ERROR_RATIO)\n        target_correct_count = MAX_TOTAL_SAMPLES - target_error_count\n        \n        # å¦‚æœæ­£ç¢ºæ¨£æœ¬éå¤šï¼Œé€²è¡Œä¸‹æ¡æ¨£\n        if len(df_correct) > target_correct_count:\n            print(f'  ğŸ“‰ å°æ­£ç¢ºæ¨£æœ¬é€²è¡Œä¸‹æ¡æ¨£: {len(df_correct)} -> {target_correct_count}')\n            df_correct = resample(df_correct, replace=False, n_samples=target_correct_count, random_state=42)\n        \n        # å°éŒ¯èª¤æ¨£æœ¬é€²è¡Œä¸Šæ¡æ¨£\n        print(f'  ğŸ“ˆ å°éŒ¯èª¤æ¨£æœ¬é€²è¡Œä¸Šæ¡æ¨£: {len(df_error)} -> {target_error_count}')\n        df_error_upsampled = resample(df_error, replace=True, n_samples=target_error_count, random_state=42)\n    else:\n        print(f'  ğŸ“ˆ å°éŒ¯èª¤æ¨£æœ¬é€²è¡Œä¸Šæ¡æ¨£: {len(df_error)} -> {needed_error_count}')\n        df_error_upsampled = resample(df_error, replace=True, n_samples=needed_error_count, random_state=42)\n    \n    # åˆä½µä¸¦æ‰“äº‚\n    df = pd.concat([df_correct, df_error_upsampled], ignore_index=True)\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    print(f'\\nğŸ‰ è³‡æ–™å¹³è¡¡å®Œæˆï¼')\n    print(f'  ç¸½ç­†æ•¸: {len(df)}')\n    print(f'  æ­£ç¢ºè¡“èª: {len(df[df[\"Correct_Output\"] == \"No issues found.\"])} ({len(df[df[\"Correct_Output\"] == \"No issues found.\"])/len(df)*100:.1f}%)')\n    print(f'  éŒ¯èª¤è¡“èª: {len(df[df[\"Correct_Output\"] != \"No issues found.\"])} ({len(df[df[\"Correct_Output\"] != \"No issues found.\"])/len(df)*100:.1f}%)')\n    \n    # æ¸…ç†è¨˜æ†¶é«”\n    del df_correct, df_error, df_error_upsampled\n    gc.collect()\nelse:\n    print(f'\\nâœ… è³‡æ–™æ¯”ä¾‹å·²ç¬¦åˆç›®æ¨™ï¼Œç„¡éœ€èª¿æ•´')\n\nprint('=' * 80)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 10: è¨­å®šè¨“ç·´åƒæ•¸ ====================\nBASE_MODEL_ID = 'google/medgemma-4b-it'\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 3\nBATCH_SIZE = 2\nMAX_LENGTH = 384\nGRADIENT_ACCUMULATION_STEPS = 8\nN_SPLITS = 5\n\nprint(f'\\nâš™ï¸ è¨“ç·´åƒæ•¸è¨­å®š')\nprint('=' * 80)\nprint(f'  æ¨¡å‹: {BASE_MODEL_ID}')\nprint(f'  äº¤å‰é©—è­‰æŠ˜æ•¸: {N_SPLITS}-fold')\nprint(f'  å­¸ç¿’ç‡: {LEARNING_RATE}')\nprint(f'  è¨“ç·´è¼ªæ•¸: {NUM_EPOCHS}')\nprint(f'  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}')\nprint(f'  æœ€å¤§åºåˆ—é•·åº¦: {MAX_LENGTH}')\nprint(f'  æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {GRADIENT_ACCUMULATION_STEPS}')\nprint(f'  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}')\nprint('=' * 80)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 11: æº–å‚™è¨“ç·´è³‡æ–™æ ¼å¼ä¸¦åˆå§‹åŒ–äº¤å‰é©—è­‰ ====================\nprint('\\nğŸ”„ æº–å‚™è¨“ç·´è³‡æ–™...')\n\ndef create_prompt(term):\n    \"\"\"å‰µå»º prompt\"\"\"\n    return f'Please check if there is any error in this medical term: {term}'\n\ndef format_example(term, response, category):\n    \"\"\"æ ¼å¼åŒ–è¨“ç·´ç¯„ä¾‹\"\"\"\n    prompt = create_prompt(term)\n    formatted_text = f'<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn>'\n    return {\n        'text': formatted_text,\n        'term': term,\n        'response': response,\n        'category': category\n    }\n\n# æº–å‚™æ‰€æœ‰è¨“ç·´ç¯„ä¾‹\nall_examples = []\nfor idx, row in df.iterrows():\n    example = format_example(\n        term=row['Term_To_Check'],\n        response=row['Correct_Output'],\n        category=row['category']\n    )\n    all_examples.append(example)\n\nprint(f'âœ… æº–å‚™äº† {len(all_examples)} å€‹è¨“ç·´ç¯„ä¾‹')\n\n# åˆå§‹åŒ–äº¤å‰é©—è­‰\nprint('\\n' + '=' * 80)\nprint('ğŸ”„ åˆå§‹åŒ– 5-Fold äº¤å‰é©—è­‰')\nprint('=' * 80)\n\nfold_results = {\n    'accuracy': [],\n    'precision': [],\n    'recall': [],\n    'f1': [],\n    'confusion_matrices': [],\n    'category_results': []\n}\n\nkfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\ndata_indices = list(range(len(all_examples)))\n\nprint(f'âœ… äº¤å‰é©—è­‰åˆå§‹åŒ–å®Œæˆ')\nprint(f'  å°‡è³‡æ–™åˆ†ç‚º {N_SPLITS} å€‹ folds')\nprint(f'  æ¯å€‹ fold ç´„æœ‰ {len(all_examples)//N_SPLITS} å€‹æ¸¬è©¦æ¨£æœ¬')\nprint('=' * 80)\n\nprint('\\nâš ï¸ é‡è¦æç¤ºï¼š')\nprint('  æ¥ä¸‹ä¾†çš„ 5 å€‹ cells å°‡åˆ†åˆ¥è¨“ç·´ 5 å€‹ folds')\nprint('  è«‹ä¾åºåŸ·è¡Œ Cell 12-16')\nprint('  æ¯å€‹ fold è¨“ç·´æ™‚é–“ç´„ 30-60 åˆ†é˜ï¼ˆå–æ±ºæ–¼è³‡æ–™é‡ï¼‰')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 12: Fold 1 è¨“ç·´ ====================\nFOLD_IDX = 0  # Fold 1 (ç´¢å¼•å¾ 0 é–‹å§‹)\n\nprint('\\n' + '=' * 80)\nprint(f'ğŸ“Š Fold {FOLD_IDX + 1}/{N_SPLITS} è¨“ç·´')\nprint('=' * 80)\n\n# ç²å–æ­¤ fold çš„è¨“ç·´å’Œæ¸¬è©¦ç´¢å¼•\ntrain_indices, test_indices = list(kfold.split(data_indices))[FOLD_IDX]\n\n# åˆ†å‰²è³‡æ–™\ntrain_examples = [all_examples[i] for i in train_indices]\ntest_examples = [all_examples[i] for i in test_indices]\n\nprint(f'  è¨“ç·´é›†å¤§å°: {len(train_examples)}')\nprint(f'  æ¸¬è©¦é›†å¤§å°: {len(test_examples)}')\n\n# è¼‰å…¥æ¨¡å‹\nprint(f'\\nğŸ”§ Fold {FOLD_IDX + 1}: è¼‰å…¥æ¨¡å‹...')\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_ID,\n    quantization_config=bnb_config,\n    device_map='auto',\n    trust_remote_code=True\n)\n\nprint('âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ')\n\n# é…ç½® LoRA\nprint(f'\\nğŸ”§ Fold {FOLD_IDX + 1}: é…ç½® LoRA...')\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n    task_type='CAUSAL_LM',\n    bias='none'\n)\n\nmodel = get_peft_model(model, lora_config)\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\nprint(f'âœ… LoRA é…ç½®å®Œæˆ')\nprint(f'  å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)')\n\n# æº–å‚™è¨“ç·´è³‡æ–™\nprint(f'\\nğŸ”„ Fold {FOLD_IDX + 1}: æº–å‚™è¨“ç·´è³‡æ–™...')\n\ndef tokenize_function(examples):\n    texts = examples['text'] if isinstance(examples['text'], list) else [examples['text']]\n    tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=None)\n    tokenized['labels'] = tokenized['input_ids'].copy()\n    return tokenized\n\ntrain_dataset_dict = {\n    'text': [ex['text'] for ex in train_examples],\n    'term': [ex['term'] for ex in train_examples],\n    'response': [ex['response'] for ex in train_examples],\n    'category': [ex['category'] for ex in train_examples]\n}\n\ntest_dataset_dict = {\n    'text': [ex['text'] for ex in test_examples],\n    'term': [ex['term'] for ex in test_examples],\n    'response': [ex['response'] for ex in test_examples],\n    'category': [ex['category'] for ex in test_examples]\n}\n\ntrain_dataset = Dataset.from_dict(train_dataset_dict)\ntest_dataset = Dataset.from_dict(test_dataset_dict)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\ntest_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\n\nprint('âœ… è³‡æ–™æº–å‚™å®Œæˆ')\n\n# æ¸…ç†è¨˜æ†¶é«”\ntorch.cuda.empty_cache()\ngc.collect()\n\n# è¨­å®šè¨“ç·´åƒæ•¸\noutput_dir = f'{output_base_dir}/fold_{FOLD_IDX + 1}'\nos.makedirs(output_dir, exist_ok=True)\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.1,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_strategy='steps',\n    save_steps=100,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    logging_steps=20,\n    logging_dir=f'{output_dir}/logs',\n    push_to_hub=False,\n    report_to='none',\n    fp16=True,\n    remove_unused_columns=False,\n    seed=42 + FOLD_IDX,\n    data_seed=42 + FOLD_IDX,\n    optim='paged_adamw_8bit',\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    gradient_checkpointing=True,\n    dataloader_num_workers=0,\n)\n\n# è¨“ç·´æ¨¡å‹\nprint(f'\\nğŸš€ Fold {FOLD_IDX + 1}: é–‹å§‹è¨“ç·´...')\nprint(f'  è¼¸å‡ºç›®éŒ„: {output_dir}')\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\ntrain_result = trainer.train()\nprint(f'\\nâœ… Fold {FOLD_IDX + 1} è¨“ç·´å®Œæˆ')\n\n# ä¿å­˜æ¨¡å‹\ntrainer.save_model(f'{output_dir}/final_model')\ntokenizer.save_pretrained(f'{output_dir}/final_model')\nprint(f'âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}/final_model')\n\n# è©•ä¼°æ¨¡å‹\nprint(f'\\nğŸ“Š Fold {FOLD_IDX + 1}: è©•ä¼°æ¸¬è©¦é›†...')\n\nmodel.eval()\npredictions = []\nground_truths = []\ncategories = []\n\nfor i, example in enumerate(test_examples):\n    if i % 50 == 0:\n        print(f'  é€²åº¦: {i}/{len(test_examples)}')\n    \n    term = example['term']\n    true_response = example['response']\n    category = example['category']\n    \n    prompt = create_prompt(term)\n    formatted = f'<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n'\n    \n    inputs = tokenizer(formatted, return_tensors='pt').to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=100,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n            do_sample=False,\n            use_cache=True\n        )\n    \n    pred_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip()\n    \n    # åˆ¤æ–·é æ¸¬çµæœ\n    pred_label = 1 if ('incorrect' in pred_text.lower() or 'error' in pred_text.lower()) else 0\n    true_label = 0 if true_response == 'No issues found.' else 1\n    \n    predictions.append(pred_label)\n    ground_truths.append(true_label)\n    categories.append(category)\n\nprint(f'  é€²åº¦: {len(test_examples)}/{len(test_examples)} âœ…')\n\n# è¨ˆç®—æŒ‡æ¨™\naccuracy = accuracy_score(ground_truths, predictions)\nprecision = precision_score(ground_truths, predictions, zero_division=0)\nrecall = recall_score(ground_truths, predictions, zero_division=0)\nf1 = f1_score(ground_truths, predictions, zero_division=0)\ncm = confusion_matrix(ground_truths, predictions)\n\n# ä¿å­˜çµæœ\nfold_results['accuracy'].append(accuracy)\nfold_results['precision'].append(precision)\nfold_results['recall'].append(recall)\nfold_results['f1'].append(f1)\nfold_results['confusion_matrices'].append(cm)\n\nprint(f'\\nğŸ“ˆ Fold {FOLD_IDX + 1} è©•ä¼°çµæœ:')\nprint(f'  Accuracy:  {accuracy:.4f}')\nprint(f'  Precision: {precision:.4f}')\nprint(f'  Recall:    {recall:.4f}')\nprint(f'  F1 Score:  {f1:.4f}')\nprint(f'\\n  æ··æ·†çŸ©é™£:')\nprint(f'  {cm}')\n\n# å„é¡åˆ¥æ€§èƒ½åˆ†æ\ncategory_metrics = {}\nunique_categories = set(categories)\n\nfor cat in unique_categories:\n    cat_indices = [i for i, c in enumerate(categories) if c == cat]\n    cat_predictions = [predictions[i] for i in cat_indices]\n    cat_ground_truths = [ground_truths[i] for i in cat_indices]\n    \n    if len(cat_indices) > 0:\n        cat_accuracy = accuracy_score(cat_ground_truths, cat_predictions)\n        cat_precision = precision_score(cat_ground_truths, cat_predictions, zero_division=0)\n        cat_recall = recall_score(cat_ground_truths, cat_predictions, zero_division=0)\n        cat_f1 = f1_score(cat_ground_truths, cat_predictions, zero_division=0)\n        \n        category_metrics[cat] = {\n            'accuracy': cat_accuracy,\n            'precision': cat_precision,\n            'recall': cat_recall,\n            'f1': cat_f1,\n            'samples': len(cat_indices)\n        }\n        \n        print(f'\\n  é¡åˆ¥ {cat}:')\n        print(f'    æ¨£æœ¬æ•¸: {len(cat_indices)}')\n        print(f'    F1: {cat_f1:.4f}, Acc: {cat_accuracy:.4f}')\n\nfold_results['category_results'].append(category_metrics)\n\n# æ¸…ç†è¨˜æ†¶é«”\ndel model, trainer, train_dataset, test_dataset\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(f'\\nâœ… Fold {FOLD_IDX + 1} å®Œæˆï¼')\nprint('=' * 80)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 13: Fold 2 è¨“ç·´ ====================\n# æ­¤ cell èˆ‡ Cell 12 çµæ§‹ç›¸åŒï¼Œåƒ… FOLD_IDX ä¸åŒ\nFOLD_IDX = 1  # Fold 2\n\nprint('\\n' + '=' * 80)\nprint(f'ğŸ“Š Fold {FOLD_IDX + 1}/{N_SPLITS} è¨“ç·´')\nprint('=' * 80)\n\ntrain_indices, test_indices = list(kfold.split(data_indices))[FOLD_IDX]\ntrain_examples = [all_examples[i] for i in train_indices]\ntest_examples = [all_examples[i] for i in test_indices]\n\nprint(f'  è¨“ç·´é›†å¤§å°: {len(train_examples)}')\nprint(f'  æ¸¬è©¦é›†å¤§å°: {len(test_examples)}')\n\nprint(f'\\nğŸ”§ Fold {FOLD_IDX + 1}: è¼‰å…¥æ¨¡å‹...')\nbnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\nmodel = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, quantization_config=bnb_config, device_map='auto', trust_remote_code=True)\nprint('âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ')\n\nprint(f'\\nğŸ”§ Fold {FOLD_IDX + 1}: é…ç½® LoRA...')\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], task_type='CAUSAL_LM', bias='none')\nmodel = get_peft_model(model, lora_config)\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\nprint(f'âœ… LoRA é…ç½®å®Œæˆ - å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)')\n\nprint(f'\\nğŸ”„ Fold {FOLD_IDX + 1}: æº–å‚™è¨“ç·´è³‡æ–™...')\ndef tokenize_function(examples):\n    texts = examples['text'] if isinstance(examples['text'], list) else [examples['text']]\n    tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=None)\n    tokenized['labels'] = tokenized['input_ids'].copy()\n    return tokenized\n\ntrain_dataset = Dataset.from_dict({'text': [ex['text'] for ex in train_examples], 'term': [ex['term'] for ex in train_examples], 'response': [ex['response'] for ex in train_examples], 'category': [ex['category'] for ex in train_examples]})\ntest_dataset = Dataset.from_dict({'text': [ex['text'] for ex in test_examples], 'term': [ex['term'] for ex in test_examples], 'response': [ex['response'] for ex in test_examples], 'category': [ex['category'] for ex in test_examples]})\ntrain_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\ntest_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\nprint('âœ… è³‡æ–™æº–å‚™å®Œæˆ')\ntorch.cuda.empty_cache()\ngc.collect()\n\noutput_dir = f'{output_base_dir}/fold_{FOLD_IDX + 1}'\nos.makedirs(output_dir, exist_ok=True)\ntraining_args = TrainingArguments(output_dir=output_dir, num_train_epochs=NUM_EPOCHS, per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, learning_rate=LEARNING_RATE, lr_scheduler_type='cosine', warmup_ratio=0.1, eval_strategy='steps', eval_steps=100, save_strategy='steps', save_steps=100, save_total_limit=1, load_best_model_at_end=True, metric_for_best_model='eval_loss', greater_is_better=False, logging_steps=20, logging_dir=f'{output_dir}/logs', push_to_hub=False, report_to='none', fp16=True, remove_unused_columns=False, seed=42 + FOLD_IDX, data_seed=42 + FOLD_IDX, optim='paged_adamw_8bit', weight_decay=0.01, max_grad_norm=1.0, gradient_checkpointing=True, dataloader_num_workers=0)\n\nprint(f'\\nğŸš€ Fold {FOLD_IDX + 1}: é–‹å§‹è¨“ç·´...')\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset)\ntrain_result = trainer.train()\nprint(f'\\nâœ… Fold {FOLD_IDX + 1} è¨“ç·´å®Œæˆ')\ntrainer.save_model(f'{output_dir}/final_model')\ntokenizer.save_pretrained(f'{output_dir}/final_model')\n\nprint(f'\\nğŸ“Š Fold {FOLD_IDX + 1}: è©•ä¼°æ¸¬è©¦é›†...')\nmodel.eval()\npredictions, ground_truths, categories = [], [], []\nfor i, example in enumerate(test_examples):\n    if i % 50 == 0:\n        print(f'  é€²åº¦: {i}/{len(test_examples)}')\n    term, true_response, category = example['term'], example['response'], example['category']\n    prompt = create_prompt(term)\n    formatted = f'<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n'\n    inputs = tokenizer(formatted, return_tensors='pt').to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, do_sample=False, use_cache=True)\n    pred_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip()\n    pred_label = 1 if ('incorrect' in pred_text.lower() or 'error' in pred_text.lower()) else 0\n    true_label = 0 if true_response == 'No issues found.' else 1\n    predictions.append(pred_label)\n    ground_truths.append(true_label)\n    categories.append(category)\n\naccuracy = accuracy_score(ground_truths, predictions)\nprecision = precision_score(ground_truths, predictions, zero_division=0)\nrecall = recall_score(ground_truths, predictions, zero_division=0)\nf1 = f1_score(ground_truths, predictions, zero_division=0)\ncm = confusion_matrix(ground_truths, predictions)\nfold_results['accuracy'].append(accuracy)\nfold_results['precision'].append(precision)\nfold_results['recall'].append(recall)\nfold_results['f1'].append(f1)\nfold_results['confusion_matrices'].append(cm)\n\nprint(f'\\nğŸ“ˆ Fold {FOLD_IDX + 1} è©•ä¼°çµæœ: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n\ncategory_metrics = {}\nfor cat in set(categories):\n    cat_indices = [i for i, c in enumerate(categories) if c == cat]\n    if len(cat_indices) > 0:\n        cat_preds = [predictions[i] for i in cat_indices]\n        cat_truths = [ground_truths[i] for i in cat_indices]\n        category_metrics[cat] = {'accuracy': accuracy_score(cat_truths, cat_preds), 'precision': precision_score(cat_truths, cat_preds, zero_division=0), 'recall': recall_score(cat_truths, cat_preds, zero_division=0), 'f1': f1_score(cat_truths, cat_preds, zero_division=0), 'samples': len(cat_indices)}\nfold_results['category_results'].append(category_metrics)\n\ndel model, trainer, train_dataset, test_dataset\ntorch.cuda.empty_cache()\ngc.collect()\nprint(f'\\nâœ… Fold {FOLD_IDX + 1} å®Œæˆï¼')\nprint('=' * 80)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 14: Fold 3 è¨“ç·´ ====================\nFOLD_IDX = 2  # Fold 3\n\nprint('\\n' + '=' * 80)\nprint(f'ğŸ“Š Fold {FOLD_IDX + 1}/{N_SPLITS} è¨“ç·´')\nprint('=' * 80)\n\ntrain_indices, test_indices = list(kfold.split(data_indices))[FOLD_IDX]\ntrain_examples = [all_examples[i] for i in train_indices]\ntest_examples = [all_examples[i] for i in test_indices]\nprint(f'  è¨“ç·´é›†å¤§å°: {len(train_examples)}, æ¸¬è©¦é›†å¤§å°: {len(test_examples)}')\n\nprint(f'\\nğŸ”§ Fold {FOLD_IDX + 1}: è¼‰å…¥æ¨¡å‹...')\nbnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\nmodel = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, quantization_config=bnb_config, device_map='auto', trust_remote_code=True)\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], task_type='CAUSAL_LM', bias='none')\nmodel = get_peft_model(model, lora_config)\nprint('âœ… æ¨¡å‹è¼‰å…¥èˆ‡ LoRA é…ç½®å®Œæˆ')\n\ndef tokenize_function(examples):\n    texts = examples['text'] if isinstance(examples['text'], list) else [examples['text']]\n    tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=None)\n    tokenized['labels'] = tokenized['input_ids'].copy()\n    return tokenized\n\ntrain_dataset = Dataset.from_dict({'text': [ex['text'] for ex in train_examples], 'term': [ex['term'] for ex in train_examples], 'response': [ex['response'] for ex in train_examples], 'category': [ex['category'] for ex in train_examples]}).map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\ntest_dataset = Dataset.from_dict({'text': [ex['text'] for ex in test_examples], 'term': [ex['term'] for ex in test_examples], 'response': [ex['response'] for ex in test_examples], 'category': [ex['category'] for ex in test_examples]}).map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\ntorch.cuda.empty_cache()\ngc.collect()\n\noutput_dir = f'{output_base_dir}/fold_{FOLD_IDX + 1}'\nos.makedirs(output_dir, exist_ok=True)\ntraining_args = TrainingArguments(output_dir=output_dir, num_train_epochs=NUM_EPOCHS, per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, learning_rate=LEARNING_RATE, lr_scheduler_type='cosine', warmup_ratio=0.1, eval_strategy='steps', eval_steps=100, save_strategy='steps', save_steps=100, save_total_limit=1, load_best_model_at_end=True, metric_for_best_model='eval_loss', greater_is_better=False, logging_steps=20, logging_dir=f'{output_dir}/logs', push_to_hub=False, report_to='none', fp16=True, remove_unused_columns=False, seed=42 + FOLD_IDX, data_seed=42 + FOLD_IDX, optim='paged_adamw_8bit', weight_decay=0.01, max_grad_norm=1.0, gradient_checkpointing=True, dataloader_num_workers=0)\n\nprint(f'\\nğŸš€ Fold {FOLD_IDX + 1}: é–‹å§‹è¨“ç·´...')\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset)\ntrainer.train()\ntrainer.save_model(f'{output_dir}/final_model')\ntokenizer.save_pretrained(f'{output_dir}/final_model')\nprint(f'âœ… Fold {FOLD_IDX + 1} è¨“ç·´å®Œæˆä¸¦ä¿å­˜')\n\nprint(f'\\nğŸ“Š Fold {FOLD_IDX + 1}: è©•ä¼°æ¸¬è©¦é›†...')\nmodel.eval()\npredictions, ground_truths, categories = [], [], []\nfor i, example in enumerate(test_examples):\n    if i % 50 == 0:\n        print(f'  é€²åº¦: {i}/{len(test_examples)}')\n    term, true_response, category = example['term'], example['response'], example['category']\n    formatted = f'<start_of_turn>user\\n{create_prompt(term)}<end_of_turn>\\n<start_of_turn>model\\n'\n    inputs = tokenizer(formatted, return_tensors='pt').to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, do_sample=False, use_cache=True)\n    pred_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip()\n    predictions.append(1 if ('incorrect' in pred_text.lower() or 'error' in pred_text.lower()) else 0)\n    ground_truths.append(0 if true_response == 'No issues found.' else 1)\n    categories.append(category)\n\naccuracy, precision, recall, f1 = accuracy_score(ground_truths, predictions), precision_score(ground_truths, predictions, zero_division=0), recall_score(ground_truths, predictions, zero_division=0), f1_score(ground_truths, predictions, zero_division=0)\ncm = confusion_matrix(ground_truths, predictions)\nfold_results['accuracy'].append(accuracy)\nfold_results['precision'].append(precision)\nfold_results['recall'].append(recall)\nfold_results['f1'].append(f1)\nfold_results['confusion_matrices'].append(cm)\nprint(f'\\nğŸ“ˆ Fold {FOLD_IDX + 1} çµæœ: Acc={accuracy:.4f}, Prec={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}')\n\ncategory_metrics = {}\nfor cat in set(categories):\n    cat_indices = [i for i, c in enumerate(categories) if c == cat]\n    if cat_indices:\n        cat_preds, cat_truths = [predictions[i] for i in cat_indices], [ground_truths[i] for i in cat_indices]\n        category_metrics[cat] = {'accuracy': accuracy_score(cat_truths, cat_preds), 'precision': precision_score(cat_truths, cat_preds, zero_division=0), 'recall': recall_score(cat_truths, cat_preds, zero_division=0), 'f1': f1_score(cat_truths, cat_preds, zero_division=0), 'samples': len(cat_indices)}\nfold_results['category_results'].append(category_metrics)\n\ndel model, trainer, train_dataset, test_dataset\ntorch.cuda.empty_cache()\ngc.collect()\nprint(f'âœ… Fold {FOLD_IDX + 1} å®Œæˆï¼\\n' + '=' * 80)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 15: Fold 4 è¨“ç·´ ====================\nFOLD_IDX = 3  # Fold 4\n\nprint('\\n' + '=' * 80)\nprint(f'ğŸ“Š Fold {FOLD_IDX + 1}/{N_SPLITS} è¨“ç·´')\nprint('=' * 80)\n\ntrain_indices, test_indices = list(kfold.split(data_indices))[FOLD_IDX]\ntrain_examples = [all_examples[i] for i in train_indices]\ntest_examples = [all_examples[i] for i in test_indices]\nprint(f'  è¨“ç·´é›†å¤§å°: {len(train_examples)}, æ¸¬è©¦é›†å¤§å°: {len(test_examples)}')\n\nprint(f'\\nğŸ”§ Fold {FOLD_IDX + 1}: è¼‰å…¥æ¨¡å‹...')\nbnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\nmodel = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, quantization_config=bnb_config, device_map='auto', trust_remote_code=True)\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], task_type='CAUSAL_LM', bias='none')\nmodel = get_peft_model(model, lora_config)\nprint('âœ… æ¨¡å‹è¼‰å…¥èˆ‡ LoRA é…ç½®å®Œæˆ')\n\ndef tokenize_function(examples):\n    texts = examples['text'] if isinstance(examples['text'], list) else [examples['text']]\n    tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=None)\n    tokenized['labels'] = tokenized['input_ids'].copy()\n    return tokenized\n\ntrain_dataset = Dataset.from_dict({'text': [ex['text'] for ex in train_examples], 'term': [ex['term'] for ex in train_examples], 'response': [ex['response'] for ex in train_examples], 'category': [ex['category'] for ex in train_examples]}).map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\ntest_dataset = Dataset.from_dict({'text': [ex['text'] for ex in test_examples], 'term': [ex['term'] for ex in test_examples], 'response': [ex['response'] for ex in test_examples], 'category': [ex['category'] for ex in test_examples]}).map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\ntorch.cuda.empty_cache()\ngc.collect()\n\noutput_dir = f'{output_base_dir}/fold_{FOLD_IDX + 1}'\nos.makedirs(output_dir, exist_ok=True)\ntraining_args = TrainingArguments(output_dir=output_dir, num_train_epochs=NUM_EPOCHS, per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, learning_rate=LEARNING_RATE, lr_scheduler_type='cosine', warmup_ratio=0.1, eval_strategy='steps', eval_steps=100, save_strategy='steps', save_steps=100, save_total_limit=1, load_best_model_at_end=True, metric_for_best_model='eval_loss', greater_is_better=False, logging_steps=20, logging_dir=f'{output_dir}/logs', push_to_hub=False, report_to='none', fp16=True, remove_unused_columns=False, seed=42 + FOLD_IDX, data_seed=42 + FOLD_IDX, optim='paged_adamw_8bit', weight_decay=0.01, max_grad_norm=1.0, gradient_checkpointing=True, dataloader_num_workers=0)\n\nprint(f'\\nğŸš€ Fold {FOLD_IDX + 1}: é–‹å§‹è¨“ç·´...')\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset)\ntrainer.train()\ntrainer.save_model(f'{output_dir}/final_model')\ntokenizer.save_pretrained(f'{output_dir}/final_model')\nprint(f'âœ… Fold {FOLD_IDX + 1} è¨“ç·´å®Œæˆä¸¦ä¿å­˜')\n\nprint(f'\\nğŸ“Š Fold {FOLD_IDX + 1}: è©•ä¼°æ¸¬è©¦é›†...')\nmodel.eval()\npredictions, ground_truths, categories = [], [], []\nfor i, example in enumerate(test_examples):\n    if i % 50 == 0:\n        print(f'  é€²åº¦: {i}/{len(test_examples)}')\n    term, true_response, category = example['term'], example['response'], example['category']\n    formatted = f'<start_of_turn>user\\n{create_prompt(term)}<end_of_turn>\\n<start_of_turn>model\\n'\n    inputs = tokenizer(formatted, return_tensors='pt').to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, do_sample=False, use_cache=True)\n    pred_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip()\n    predictions.append(1 if ('incorrect' in pred_text.lower() or 'error' in pred_text.lower()) else 0)\n    ground_truths.append(0 if true_response == 'No issues found.' else 1)\n    categories.append(category)\n\naccuracy, precision, recall, f1 = accuracy_score(ground_truths, predictions), precision_score(ground_truths, predictions, zero_division=0), recall_score(ground_truths, predictions, zero_division=0), f1_score(ground_truths, predictions, zero_division=0)\ncm = confusion_matrix(ground_truths, predictions)\nfold_results['accuracy'].append(accuracy)\nfold_results['precision'].append(precision)\nfold_results['recall'].append(recall)\nfold_results['f1'].append(f1)\nfold_results['confusion_matrices'].append(cm)\nprint(f'\\nğŸ“ˆ Fold {FOLD_IDX + 1} çµæœ: Acc={accuracy:.4f}, Prec={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}')\n\ncategory_metrics = {}\nfor cat in set(categories):\n    cat_indices = [i for i, c in enumerate(categories) if c == cat]\n    if cat_indices:\n        cat_preds, cat_truths = [predictions[i] for i in cat_indices], [ground_truths[i] for i in cat_indices]\n        category_metrics[cat] = {'accuracy': accuracy_score(cat_truths, cat_preds), 'precision': precision_score(cat_truths, cat_preds, zero_division=0), 'recall': recall_score(cat_truths, cat_preds, zero_division=0), 'f1': f1_score(cat_truths, cat_preds, zero_division=0), 'samples': len(cat_indices)}\nfold_results['category_results'].append(category_metrics)\n\ndel model, trainer, train_dataset, test_dataset\ntorch.cuda.empty_cache()\ngc.collect()\nprint(f'âœ… Fold {FOLD_IDX + 1} å®Œæˆï¼\\n' + '=' * 80)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 16: Fold 5 è¨“ç·´ ====================\nFOLD_IDX = 4  # Fold 5\n\nprint('\\n' + '=' * 80)\nprint(f'ğŸ“Š Fold {FOLD_IDX + 1}/{N_SPLITS} è¨“ç·´ (æœ€å¾Œä¸€å€‹ Fold)')\nprint('=' * 80)\n\ntrain_indices, test_indices = list(kfold.split(data_indices))[FOLD_IDX]\ntrain_examples = [all_examples[i] for i in train_indices]\ntest_examples = [all_examples[i] for i in test_indices]\nprint(f'  è¨“ç·´é›†å¤§å°: {len(train_examples)}, æ¸¬è©¦é›†å¤§å°: {len(test_examples)}')\n\nprint(f'\\nğŸ”§ Fold {FOLD_IDX + 1}: è¼‰å…¥æ¨¡å‹...')\nbnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\nmodel = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, quantization_config=bnb_config, device_map='auto', trust_remote_code=True)\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], task_type='CAUSAL_LM', bias='none')\nmodel = get_peft_model(model, lora_config)\nprint('âœ… æ¨¡å‹è¼‰å…¥èˆ‡ LoRA é…ç½®å®Œæˆ')\n\ndef tokenize_function(examples):\n    texts = examples['text'] if isinstance(examples['text'], list) else [examples['text']]\n    tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=None)\n    tokenized['labels'] = tokenized['input_ids'].copy()\n    return tokenized\n\ntrain_dataset = Dataset.from_dict({'text': [ex['text'] for ex in train_examples], 'term': [ex['term'] for ex in train_examples], 'response': [ex['response'] for ex in train_examples], 'category': [ex['category'] for ex in train_examples]}).map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\ntest_dataset = Dataset.from_dict({'text': [ex['text'] for ex in test_examples], 'term': [ex['term'] for ex in test_examples], 'response': [ex['response'] for ex in test_examples], 'category': [ex['category'] for ex in test_examples]}).map(tokenize_function, batched=True, remove_columns=['text', 'term', 'response', 'category'], batch_size=100)\ntorch.cuda.empty_cache()\ngc.collect()\n\noutput_dir = f'{output_base_dir}/fold_{FOLD_IDX + 1}'\nos.makedirs(output_dir, exist_ok=True)\ntraining_args = TrainingArguments(output_dir=output_dir, num_train_epochs=NUM_EPOCHS, per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE, gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, learning_rate=LEARNING_RATE, lr_scheduler_type='cosine', warmup_ratio=0.1, eval_strategy='steps', eval_steps=100, save_strategy='steps', save_steps=100, save_total_limit=1, load_best_model_at_end=True, metric_for_best_model='eval_loss', greater_is_better=False, logging_steps=20, logging_dir=f'{output_dir}/logs', push_to_hub=False, report_to='none', fp16=True, remove_unused_columns=False, seed=42 + FOLD_IDX, data_seed=42 + FOLD_IDX, optim='paged_adamw_8bit', weight_decay=0.01, max_grad_norm=1.0, gradient_checkpointing=True, dataloader_num_workers=0)\n\nprint(f'\\nğŸš€ Fold {FOLD_IDX + 1}: é–‹å§‹è¨“ç·´...')\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset)\ntrainer.train()\ntrainer.save_model(f'{output_dir}/final_model')\ntokenizer.save_pretrained(f'{output_dir}/final_model')\nprint(f'âœ… Fold {FOLD_IDX + 1} è¨“ç·´å®Œæˆä¸¦ä¿å­˜')\n\nprint(f'\\nğŸ“Š Fold {FOLD_IDX + 1}: è©•ä¼°æ¸¬è©¦é›†...')\nmodel.eval()\npredictions, ground_truths, categories = [], [], []\nfor i, example in enumerate(test_examples):\n    if i % 50 == 0:\n        print(f'  é€²åº¦: {i}/{len(test_examples)}')\n    term, true_response, category = example['term'], example['response'], example['category']\n    formatted = f'<start_of_turn>user\\n{create_prompt(term)}<end_of_turn>\\n<start_of_turn>model\\n'\n    inputs = tokenizer(formatted, return_tensors='pt').to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, do_sample=False, use_cache=True)\n    pred_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip()\n    predictions.append(1 if ('incorrect' in pred_text.lower() or 'error' in pred_text.lower()) else 0)\n    ground_truths.append(0 if true_response == 'No issues found.' else 1)\n    categories.append(category)\n\naccuracy, precision, recall, f1 = accuracy_score(ground_truths, predictions), precision_score(ground_truths, predictions, zero_division=0), recall_score(ground_truths, predictions, zero_division=0), f1_score(ground_truths, predictions, zero_division=0)\ncm = confusion_matrix(ground_truths, predictions)\nfold_results['accuracy'].append(accuracy)\nfold_results['precision'].append(precision)\nfold_results['recall'].append(recall)\nfold_results['f1'].append(f1)\nfold_results['confusion_matrices'].append(cm)\nprint(f'\\nğŸ“ˆ Fold {FOLD_IDX + 1} çµæœ: Acc={accuracy:.4f}, Prec={precision:.4f}, Rec={recall:.4f}, F1={f1:.4f}')\n\ncategory_metrics = {}\nfor cat in set(categories):\n    cat_indices = [i for i, c in enumerate(categories) if c == cat]\n    if cat_indices:\n        cat_preds, cat_truths = [predictions[i] for i in cat_indices], [ground_truths[i] for i in cat_indices]\n        category_metrics[cat] = {'accuracy': accuracy_score(cat_truths, cat_preds), 'precision': precision_score(cat_truths, cat_preds, zero_division=0), 'recall': recall_score(cat_truths, cat_preds, zero_division=0), 'f1': f1_score(cat_truths, cat_preds, zero_division=0), 'samples': len(cat_indices)}\nfold_results['category_results'].append(category_metrics)\n\ndel model, trainer, train_dataset, test_dataset\ntorch.cuda.empty_cache()\ngc.collect()\nprint(f'âœ… Fold {FOLD_IDX + 1} å®Œæˆï¼')\nprint('=' * 80)\nprint('\\nğŸ‰ æ‰€æœ‰ 5 å€‹ Folds è¨“ç·´å®Œæˆï¼è«‹ç¹¼çºŒåŸ·è¡Œå¾ŒçºŒçš„çµ±è¨ˆåˆ†æ cellsã€‚')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 17: çµ±è¨ˆåˆ†æ ====================\nprint('\\n' + '=' * 80)\nprint('ğŸ“Š äº¤å‰é©—è­‰çµ±è¨ˆåˆ†æ')\nprint('=' * 80)\n\n# ç¢ºä¿æ‰€æœ‰ 5 å€‹ folds éƒ½å·²å®Œæˆ\nif len(fold_results['f1']) != N_SPLITS:\n    print(f'\\nâš ï¸ è­¦å‘Šï¼šç›®å‰åªå®Œæˆäº† {len(fold_results[\"f1\"])} å€‹ foldsï¼Œéœ€è¦å®Œæˆ {N_SPLITS} å€‹ï¼')\n    print('è«‹ç¢ºä¿å·²åŸ·è¡Œæ‰€æœ‰ fold è¨“ç·´ cells (Cell 12-16)')\nelse:\n    print(f'\\nâœ… æ‰€æœ‰ {N_SPLITS} å€‹ folds å·²å®Œæˆ')\n\nmetrics_stats = {}\nfor metric in ['accuracy', 'precision', 'recall', 'f1']:\n    values = fold_results[metric]\n    mean = np.mean(values)\n    std = np.std(values, ddof=1)\n    \n    # è¨ˆç®— 95% ä¿¡è³´å€é–“\n    confidence_level = 0.95\n    degrees_freedom = len(values) - 1\n    confidence_interval = stats.t.interval(confidence_level, degrees_freedom, mean, stats.sem(values))\n    \n    metrics_stats[metric] = {\n        'mean': mean,\n        'std': std,\n        'ci_lower': confidence_interval[0],\n        'ci_upper': confidence_interval[1],\n        'values': values\n    }\n    \n    print(f'\\n{metric.upper()}:')\n    print(f'  å„ Fold çµæœ: {[f\"{v:.4f}\" for v in values]}')\n    print(f'  å¹³å‡å€¼: {mean:.4f}')\n    print(f'  æ¨™æº–å·®: {std:.4f}')\n    print(f'  95% ä¿¡è³´å€é–“: [{confidence_interval[0]:.4f}, {confidence_interval[1]:.4f}]')\n\nprint('\\n' + '=' * 80)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 18: æ··æ·†çŸ©é™£è¦–è¦ºåŒ– ====================\nprint('\\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é™£è¦–è¦ºåŒ–...')\n\n# è¨ˆç®—å¹³å‡æ··æ·†çŸ©é™£\navg_cm = np.mean(fold_results['confusion_matrices'], axis=0)\n\n# å‰µå»ºåœ–è¡¨\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('5-Fold Cross Validation - Confusion Matrices', fontsize=16, fontweight='bold')\n\n# ç¹ªè£½æ¯å€‹ fold çš„æ··æ·†çŸ©é™£\nfor idx in range(N_SPLITS):\n    row = idx // 3\n    col = idx % 3\n    ax = axes[row, col]\n    \n    cm = fold_results['confusion_matrices'][idx]\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n                xticklabels=['Correct', 'Error'],\n                yticklabels=['Correct', 'Error'],\n                cbar_kws={'label': 'Count'})\n    ax.set_title(f'Fold {idx + 1}\\nF1={fold_results[\"f1\"][idx]:.4f}', fontsize=12)\n    ax.set_ylabel('True Label')\n    ax.set_xlabel('Predicted Label')\n\n# ç¹ªè£½å¹³å‡æ··æ·†çŸ©é™£\nax = axes[1, 2]\nsns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Greens', ax=ax,\n            xticklabels=['Correct', 'Error'],\n            yticklabels=['Correct', 'Error'],\n            cbar_kws={'label': 'Average Count'})\nax.set_title(f'Average Confusion Matrix\\nF1={metrics_stats[\"f1\"][\"mean\"]:.4f}Â±{metrics_stats[\"f1\"][\"std\"]:.4f}', \n             fontsize=12, fontweight='bold')\nax.set_ylabel('True Label')\nax.set_xlabel('Predicted Label')\n\nplt.tight_layout()\n\n# ä¿å­˜åœ–è¡¨\noutput_path = f'{output_base_dir}/confusion_matrices_cv.png'\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f'âœ… æ··æ·†çŸ©é™£å·²ä¿å­˜è‡³: {output_path}')\nplt.show()\n\nprint('\\næ··æ·†çŸ©é™£å·²é¡¯ç¤ºå®Œæˆ')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 19: æ€§èƒ½æŒ‡æ¨™è¦–è¦ºåŒ– ====================\nprint('\\nğŸ“Š ç”Ÿæˆæ€§èƒ½æŒ‡æ¨™è¦–è¦ºåŒ–...')\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('5-Fold Cross Validation - Performance Metrics', fontsize=16, fontweight='bold')\n\nmetrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\nmetric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n\nfor idx, (metric, name) in enumerate(zip(metrics_to_plot, metric_names)):\n    row = idx // 2\n    col = idx % 2\n    ax = axes[row, col]\n    \n    values = metrics_stats[metric]['values']\n    mean = metrics_stats[metric]['mean']\n    ci_lower = metrics_stats[metric]['ci_lower']\n    ci_upper = metrics_stats[metric]['ci_upper']\n    \n    folds = list(range(1, N_SPLITS + 1))\n    \n    # ç¹ªè£½æŠ˜ç·šåœ–\n    ax.plot(folds, values, marker='o', linewidth=2, markersize=8, \n            label='Fold Results', color='steelblue')\n    \n    # ç¹ªè£½å¹³å‡å€¼ç·š\n    ax.axhline(y=mean, color='red', linestyle='--', linewidth=2, \n               label=f'Mean: {mean:.4f}')\n    \n    # ç¹ªè£½ä¿¡è³´å€é–“\n    ax.axhspan(ci_lower, ci_upper, alpha=0.2, color='red', \n               label=f'95% CI')\n    \n    ax.set_xlabel('Fold', fontsize=12)\n    ax.set_ylabel(name, fontsize=12)\n    ax.set_title(f'{name}\\nMean: {mean:.4f} Â± {metrics_stats[metric][\"std\"]:.4f}', \n                 fontsize=12, fontweight='bold')\n    ax.set_xticks(folds)\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='best')\n    \n    # è¨­å®š y è»¸ç¯„åœ\n    y_min = max(0.0, min(values) - 0.05)\n    y_max = min(1.0, max(values) + 0.05)\n    ax.set_ylim([y_min, y_max])\n\nplt.tight_layout()\n\n# ä¿å­˜åœ–è¡¨\noutput_path = f'{output_base_dir}/performance_metrics_cv.png'\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\nprint(f'âœ… æ€§èƒ½æŒ‡æ¨™åœ–å·²ä¿å­˜è‡³: {output_path}')\nplt.show()\n\nprint('\\næ€§èƒ½æŒ‡æ¨™è¦–è¦ºåŒ–å·²å®Œæˆ')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ==================== CELL 20: ç”Ÿæˆè©³ç´°å ±å‘Š ====================\nprint('\\nğŸ“ ç”Ÿæˆè©³ç´°å ±å‘Š...')\n\nreport_lines = []\nreport_lines.append('=' * 80)\nreport_lines.append('MedGemma é†«ç™‚è¡“èªæ ¡æ­£æ¨¡å‹ - 5-Fold äº¤å‰é©—è­‰å ±å‘Š')\nreport_lines.append('=' * 80)\nreport_lines.append('')\n\n# åŸºæœ¬è³‡è¨Š\nreport_lines.append('ã€åŸºæœ¬è³‡è¨Šã€‘')\nreport_lines.append(f'  æ¨¡å‹: {BASE_MODEL_ID}')\nreport_lines.append(f'  è¨“ç·´è³‡æ–™ç¸½æ•¸: {len(all_examples)}')\nreport_lines.append(f'  äº¤å‰é©—è­‰æŠ˜æ•¸: {N_SPLITS}')\nreport_lines.append(f'  è¨“ç·´è¼ªæ•¸: {NUM_EPOCHS}')\nreport_lines.append(f'  å­¸ç¿’ç‡: {LEARNING_RATE}')\nreport_lines.append(f'  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}')\nreport_lines.append(f'  æ¢¯åº¦ç´¯ç©æ­¥æ•¸: {GRADIENT_ACCUMULATION_STEPS}')\nif taiwan_drug_names:\n    report_lines.append(f'  å°ç£è—¥ç‰©åç¨±æ•¸é‡: {len(taiwan_drug_names)}')\nreport_lines.append('')\n\n# æ•´é«”æ€§èƒ½çµ±è¨ˆ\nreport_lines.append('ã€æ•´é«”æ€§èƒ½çµ±è¨ˆã€‘')\nfor metric, name in zip(['accuracy', 'precision', 'recall', 'f1'],\n                       ['Accuracy', 'Precision', 'Recall', 'F1 Score']):\n    stats_data = metrics_stats[metric]\n    report_lines.append(f'\\n{name}:')\n    report_lines.append(f'  å¹³å‡å€¼: {stats_data[\"mean\"]:.4f}')\n    report_lines.append(f'  æ¨™æº–å·®: {stats_data[\"std\"]:.4f}')\n    report_lines.append(f'  95% ä¿¡è³´å€é–“: [{stats_data[\"ci_lower\"]:.4f}, {stats_data[\"ci_upper\"]:.4f}]')\n    report_lines.append(f'  å„ Fold çµæœ: {[f\"{v:.4f}\" for v in stats_data[\"values\"]]}')\n\n# å„ Fold è©³ç´°çµæœ\nreport_lines.append('\\n' + '=' * 80)\nreport_lines.append('ã€å„ Fold è©³ç´°çµæœã€‘')\nfor i in range(N_SPLITS):\n    report_lines.append(f'\\nFold {i+1}:')\n    report_lines.append(f'  Accuracy:  {fold_results[\"accuracy\"][i]:.4f}')\n    report_lines.append(f'  Precision: {fold_results[\"precision\"][i]:.4f}')\n    report_lines.append(f'  Recall:    {fold_results[\"recall\"][i]:.4f}')\n    report_lines.append(f'  F1 Score:  {fold_results[\"f1\"][i]:.4f}')\n    report_lines.append(f'  æ··æ·†çŸ©é™£:\\n  {fold_results[\"confusion_matrices\"][i]}')\n\n# çµè«–\nreport_lines.append('\\n' + '=' * 80)\nreport_lines.append('ã€çµè«–ã€‘')\nreport_lines.append(f'æœ¬æ¬¡ 5-Fold äº¤å‰é©—è­‰è¨“ç·´å·²å®Œæˆã€‚')\nreport_lines.append(f'å¹³å‡ F1 Score: {metrics_stats[\"f1\"][\"mean\"]:.4f} Â± {metrics_stats[\"f1\"][\"std\"]:.4f}')\nreport_lines.append(f'å¹³å‡ Accuracy: {metrics_stats[\"accuracy\"][\"mean\"]:.4f} Â± {metrics_stats[\"accuracy\"][\"std\"]:.4f}')\nreport_lines.append('')\nreport_lines.append('æ¨¡å‹æª”æ¡ˆä¿å­˜ä½ç½®:')\nfor i in range(N_SPLITS):\n    report_lines.append(f'  Fold {i+1}: {output_base_dir}/fold_{i+1}/final_model')\nreport_lines.append('')\nreport_lines.append('è¦–è¦ºåŒ–åœ–è¡¨ä¿å­˜ä½ç½®:')\nreport_lines.append(f'  æ··æ·†çŸ©é™£: {output_base_dir}/confusion_matrices_cv.png')\nreport_lines.append(f'  æ€§èƒ½æŒ‡æ¨™: {output_base_dir}/performance_metrics_cv.png')\nreport_lines.append('=' * 80)\n\n# ç”Ÿæˆå ±å‘Šæ–‡å­—\nreport_text = '\\n'.join(report_lines)\n\n# ä¿å­˜å ±å‘Š\nreport_path = f'{output_base_dir}/cv_report.txt'\nwith open(report_path, 'w', encoding='utf-8') as f:\n    f.write(report_text)\n\nprint(f'âœ… è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_path}')\nprint('\\n' + '=' * 80)\nprint('å ±å‘Šå…§å®¹:')\nprint('=' * 80)\nprint(report_text)\n\nprint('\\n' + '=' * 80)\nprint('ğŸ‰ 5-Fold äº¤å‰é©—è­‰å®Œæˆï¼')\nprint('=' * 80)\nprint(f'\\nğŸ“Š æœ€çµ‚çµæœæ‘˜è¦:')\nprint(f'  F1 Score:  {metrics_stats[\"f1\"][\"mean\"]:.4f} Â± {metrics_stats[\"f1\"][\"std\"]:.4f}')\nprint(f'  Accuracy:  {metrics_stats[\"accuracy\"][\"mean\"]:.4f} Â± {metrics_stats[\"accuracy\"][\"std\"]:.4f}')\nprint(f'  Precision: {metrics_stats[\"precision\"][\"mean\"]:.4f} Â± {metrics_stats[\"precision\"][\"std\"]:.4f}')\nprint(f'  Recall:    {metrics_stats[\"recall\"][\"mean\"]:.4f} Â± {metrics_stats[\"recall\"][\"std\"]:.4f}')\nprint('\\næ‰€æœ‰çµæœå·²ä¿å­˜è‡³ Google Drive')\nprint(f'è¼¸å‡ºç›®éŒ„: {output_base_dir}')\nprint('=' * 80)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}